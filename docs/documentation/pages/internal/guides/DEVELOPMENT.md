# Разработка Deckhouse

- Версия, релиз (существительное) — логически законченная и анонсированная [версия](#версионирование) программы.
В качестве существительного, употребление терминов `версия` и `релиз` — равносильно.
- "Код" какой-либо ветки не являющийся релизным, считается dev-версией.
- Релиз (глагол) — процесс создания, выпуска, анонсирования новой версии (релиза).
*Не путать* со сменой версии на канале обновлений.
- Канал обновления — `alpha`, `beta`, `early-access`, `stable` и `rock-solid`, в порядке повышения стабильности (см. ниже подробнее). Фраза "канал обновления early-access и выше", подразумевает каналы - `early-access`, `stable` и `rock-solid`, и т.п.

Версионирование
----------------
Формат соответствует [semver](https://semver.org/).

Примеры
- `v1.24.0`, `v1.24.5` - версии, распространяемые на каналы обновлений early-access и "выше".
- `v1.24.10-beta`, `v1.24.32-beta.4` - версии, распространяемые на каналы обновлений "не выше" beta.
- `v1.24.10-alpha`, `v1.24.32-alpha.4` - версии, распространяемые на каналы обновлений "не выше" alpha.

Как проверить работу Deckhouse
------------------------------
CI настроен так, что каждый branch всегда собирается в образ и доступен по адресу `registry.flant.com/sys/antiopa/dev:<BRANCH>`.
Все что нужно, чтобы проверить dev-версию — изменить образ в deployment'е Deckhouse.

Автообновление
--------------
При commit'е изменений в git происходит сборка нового docker образа Deckhouse.

Работающая в кластере копия Deckhouse периодически проверяет наличие нового образа в docker-registry (новый digest для того же tag'а образа).
Если digest для tag'а в registry не соответствует digest'у образа в кластере, Deckhouse изменяет manifest своего deployment'а и завершает работу.
Обновление образа из registry проходит при создании нового pod'а с Deckhouse.

Процесс релиза новых версий и смены версий на каналах обновлений
----------------------------------------------------------------
Получение информации о версиях (релизах), находящихся на каналах обновлений в текущий момент:
- Issue и MR которые вошли в конкретную версию находятся в milestone c названием версии.
- Информация о версиях (релизах) размещается в Slack в канале `#deckhouse-releases`, также эта информация находится в описании тэга, соответствующего версии.
- Код, соответствующий версии выкаченной в настоящий момент на конкретный канал обновлений, находится в соответствующей ветке (`alpha`, `beta` и т.д.).

### Релиз версии с **новым** функционалом/изменениями и смена версий на каналах обновлений

Issue и MR, которые должны войти в ближайший релиз, находятся в milestone ***current***.

Когда в milestone ***current*** набралось достаточно изменений, чтобы выпустить новый релиз, необходимо сделать следующее:
1. Подготовительные действия
    1. Определить `версию релиза` в формате `1.XX`.
    1. Cоздать milestone с будущим названием релиза, перенести в него все issue и MR, которые попадут в этот релиз (milestone не нужно переименовывать, — нужно именно создать новый и в него перенести issue и MR).
    1. В поле Description составить подробное описание изменений, предназначенное, в первую очередь, для DevOps команд (чтобы они могли четко понимать, какие изменения есть в релизе).
    1. Создать релизный branch в формате `release-<версия релиза>` (например — `release-1.24`) от соответствующего commit'а в `master`.
    1. При необходимости сделать cherry-pick commit'ов которые должны попасть в релиз, но не попали в релизный branch.
    1. Создать английскую и русскую версию поста в Slack (именно создать в slack'е пост — Add -> Post). В пост вставить и отформатировать содержимое из описания milestone. Поставить checkbox `Create a public link to share outside of Slack`.
1. Выпуск версии
    1. Предварительный выкат
        1. Сделать сообщение в канал `#deckhouse-releases` с объявлением, в котором вставить ссылку на русскоязычный пост. В объявлении написать, что выпущен новый релиз и на канале обновлений `alpha` произойдет смена версии. Дернуть все команды (чтобы они узнали о выходе нового релиза).
        1. Сделать рассылку в клиентские каналы проектов, кластеры которых будут затронуты при выкате (#TODO описание процесса рассылки).
        1. Выкатить на `alpha` — задание `Alpha (pre-release)` стадии `Deploy`. При этом werf делает push образа, который уже есть в `antiopa/dev:<версия релизного бранча>`, в `antiopa:alpha` и все инсталляции, подключенные к каналу обновлений `alpha` — [обновляются](#автообновление).
        1. Проверить логи на достаточном наборе кластеров подключенных к каналу обновлений `alpha`.
        1. Если при выкате на `alpha` произошли ошибки:
            1. Сразу сообщить о проблеме в треде к сообщению о выкате (в канале Slack `#deckhouse-releases`) и дернуть дежурного L1 и дежурного команды чей кластер. Сообщить что проблемой занимается R&D.
            1. (опционально) Заводим issue.
            1. Исправляем в MR, принимаем эти MR.
            1. Сообщаем об изменениях в тредах канала `#deckhouse-releases` и рассылаем клиентам уведомления (#TODO описание процесса рассылки) (никого НЕ дергаем).
            1. Снова выкатываем на `alpha` — задание `Alpha (pre-release)` стадии `Deploy`.
            1. Повторяем, пока не будет все хорошо.
            1. Рассылаем клиентам уведомления об окончании выката.
        1. Ждем следующего дня.
    1. Фиксация версии
        1. Создать tag `v<версия релиза>-alpha` на соответствующем commit'е релизного branch'а (как правило это последний commit). В раздел Release Notes (а не Message) описания тега перенести описание из milestone релиза, в самом milestone его можно удалить, оставив ссылку на соответствующий тэг. CI настроен так, что в registry произойдет push образа `antiopa:<tag>`.
        1. Окончательно выкатить на `alpha` — запустить задание `alpha` стадии `Deploy`. CI настроен так, что при этом werf делает push образа, который уже есть в `antiopa:<tag>`, в `antiopa:alpha`.
1. Смена версии (релиза) на `beta`, `early-access`, `stable` и `rock-solid`.
    1. Уведомить в Slack.
        1. `beta` – Создать tag `v<версия релиза>-beta`, создать отдельное сообщение в канале `#deckhouse-releases` в котором вставить ссылку на русскоязычный пост. Дернуть только дежурных команд. Сделать рассылку в клиентские каналы проектов, кластеры которых будут затронуты при выкате.
        1. `early-access`:
            1. Создать tag `v<версия релиза>`
            1. За два часа до смены версии создать отдельное сообщение:
               - в канале `#deckhouse-releases`, сообщить список кластеров, дернуть дежурных команд.
               - сделать рассылку в клиентские каналы проектов, кластеры которых будут затронуты при выкате (#TODO описание процесса рассылки).
            1. Прямо перед сменой версии дернуть дежурных команд и ответственного за релиз инженера, сообщить список кластеров (если он изменился за два часа).
        1. `stable` и `rock-solid`:
            1. За один день до смены версии создать отдельное сообщение:
               - в канале `#deckhouse-releases`, сообщить список кластеров, никого не дергать.
               - сделать рассылку в клиентские каналы проектов, кластеры которых будут затронуты при выкате (#TODO описание процесса рассылки).
            1. Прямо перед сменой версии дернуть команды, сообщить список серверов.
    1. Сменить версию на соответствующем канале обновлений (`beta`, `early-access`, `stable` или `rock-solid`), для чего запустить задание с соответствующим именем стадии `Deploy`. CI настроен так, что при этом Werf делает push образа, который уже есть в `antiopa:<tag>`, в `antiopa:<канал обновления>`.
    1. Централизовано проверить логи на всех обновленных кластерах на предмет наличия ошибок.
    1. Сделать рассылку в клиентские каналы проектов затронутых релизом сообщение об окончании выката.

Периодичность и время смены версий на каналах обновлений:
1. Смена версии на `alpha` выполняется в любое удобное для команды R&D время, с любой периодичность и без предварительного предупреждения.
1. Смена версии на `beta` выполняется в любое удобное для команды R&D время, с любой периодичность и без предварительного предупреждения, но не ранее чем на следующий день после смены версии на `alpha`
1. Смена версий на `early-access`, `stable` и `rock-solid` выполняется **только** в интервале **11:30—13:00 GMT+3** и только в определенные дни недели:
   1. `rock-solid` — по вторникам, но не ранее чем на 13-й день, после смены на эту версию на канале `stable`.
   1. `stable` — по средам, но не ранее чем на 6-й день, после смены на эту версию на канале `early-access`.
   1. `early-access` — по четвергам, но не ранее чем на следующий день после смены на эту версию на канале `beta`.

### Релиз версии с **исправлениями** (hotfix) и смена версий на каналах обновлений

Если так получилось, что в релизе уже выехавшем на `beta` (или далее) обнаружена ошибка, то мы разделяем ряд случаев:
1. Ошибка находится только в новом функционале:
    1. В релизе только новый функционал, которым пока никто не пользуется – смена версий на такую версию останавливается (этот релиз не поедет на следующие каналы обновлений), о чем сообщается в тредах. Исправление будет в следующих релизах.
    1. В релизе есть и другие изменения, которые срочно нужны, или новый функционал срочно нужен клиентам – выполняются hotfix-release'ы.
1. Ошибка в существующем функционале – выполняются hotfix-release'ы.

Hotfix-релизы это не самостоятельные релизы, а набор изменений, который внеочередно бэкпортится во все активные релизы (при необходимости). После выхода hotfix-релиза, основная версия не меняется, но получает суффикс. Т.к. эти изменения бэкпортятся во все активные релизы, эти изменения должны быть минимально допустимыми!

**Если есть возможность потерпеть — всегда лучше потерпеть и дождаться, пока изменение доедет основным релизным процессом. Особенно это касается каналов обновлений `stable` и `rock-solid`.**

Выпуск hotfix-релизов и смена версий:
1. Подготовительные действия:
    1. MR'ы содержащие исправления должны иметь специальный лейбл `Type: Hotfix`.
    1. Определить `версию hotfix-релиза` в формате `v1.XX.YY` (где YY, это номер hotfix-релиза, начиная с 1).
    1. Cоздать milestone с будущим названием hotfix-релиза, перенести в него все issue и MR, которые попадут в этот hotfix-релиз (milestone не нужно переименовывать, — нужно именно создать новый и в него перенести issue и MR).
    1. В поле Description составить подробное описание изменений, предназначенное, в первую очередь, для DevOps команд (чтобы они могли четко понимать, какие изменения есть в релизе), но читаемые также и клиентом.
    1. Создать английскую и русскую версию поста в Slack (именно создать в slack'е пост — Add -> Post) с учетом описания milestone. Поставить checkbox `Create a public link to share outside of Slack`.
    1. Выпустить версию с исправлениями (включая шаги по предварительному выкату и фиксации).
1. Для каналов обновлений `beta`, `early-access`, `stable` и `rock-solid`, на которые необходимо выкатить изменения, выполнить следующее:
    1. Выполнить cherry-pick изменений из всех MR'ов hotfix-релиза в соответсвующий релизный branch.
    1. Поставить тег в формате `<версия hotfix-релиза>-alpha` (например: `v1.24.3-alpha`).
    1. Сделать сообщение в канал `#deckhouse-releases` с объявлением, в котором вставить ссылку на пост.
    1. Произвести смену версии аналогично обычному процессу, создавая соответствующие тэги (например: `v1.24.3-alpha` -> `v1.24.3-beta` -> `v1.24.3-early-access`).
    1. В случае проблем с хотфиксом и необходимости выпустить изменения, к названию тэга соответствующего версии проблемного hotfix-релиза добавляется суффикс '.XX' (Например, изменения доехали до канала обновлений beta, с тэгом `v1.24.3-beta`. Тэг с изменениями будет `v1.24.3-beta.1` и т.п)
1. После того, как релиз ушел с канала `rock-solid` (следующий релиз уже был выкачен на `rock-solid`), релизный бранч удаляется.

Периодичность и время смены версий на hotfix-версии на каналах обновлений **в пределах одной и той же основной версии**:
1. Смена на hotfix-версию на канале `alpha` выполняется в любое удобное для команды R&D время, с любой периодичность и без предварительного предупреждения.
1. Смена на hotfix-версию на каналах `beta` и `early-access` выполняется в любое удобное для команды R&D время, но не ранее чем через 2 часа после смены на эту версию на `alpha` и `beta`, соответственно.
1. Смена на hotfix-версию на каналах `stable` и `rock-solid` выполняется в интервале **13:00—14:00 GMT+3** (без ограничения дней):
   1. `stable` — не ранее, чем на следующий день после смены на `early-access`.
   1. `rock-solid` — не ранее, чем на 6-й день после смены на `stable`.
1. В случае **реальной необходимости** внесения срочных изменений (критический баг, или критическая уязвимость) и **по согласованию с тимлидами**, эти правила могут быть нарушены.

### Остановка запланированной смены версии

1. Если в релизе (или hotfix-релизе) обнаруживается деградация существовавшего ранее функционала, то плановая смена на эту версию приостанавливается.
1. Если для исправления деградации выпускается другая версия с исправлениями (hotfix-релиз), то действия выполняются согласно предыдущему разделу. И если hotfix-релиз оказывается успешным, то текущий релиз с исправлениями, продолжает свой путь далее по каналам обновлений, согласно уровню стабильности канала.
1. Если было принято решение не выпускать hotfix-релиз, а дождаться исправлений вместе со следующим релизом, то текущий релиз считается остановленным. Остановленый релиз — это релиз, на который больше не производится смена версии в каналах обновлений (нет никакого смысла менять версию в канале на заведомо содержащую деградацию).
1. Для остановленных релизов необходимо произвести следующие действия:
    1. В тред релиза сообщить, что этот релиз остановлен и смена версии в каналах обновлений на эту версию больше производиться не будет.
    1. Указать примерную дату выходя следующего релиза, который заменит этот, если такая дата известна.
    1. При выпуске следующего релиза, в релиз-месседже (в том числе в fox) указать, следующую фразу: "Настоящий релиз содержит все изменения релиза XXXXXXXXX, который был остановлен на `early-access`". Если релизов несколько, то перечислить каждый.

{% offtopic title="Использование ***REMOVED*** при релизе" %}
{% raw %}

- Получить информацию о версии (бранче) Deckhouse на каждом кластере:
  ```bash
  ./***REMOVED*** -s "if :kubectl: get ns/d8-system 2> /dev/null > /dev/null ; then :kubectl: -n d8-system get deploy/deckhouse -o json | jq '.spec.template.spec.containers[0].image' -r; else echo "---"; fi" | tee /tmp/res
  ```
  В результате — в /tmp/res будет список кластеров с текущими каналами обновлений или версиями. Дальнейшие команды приводятся исходя из использования /tmp/res.

- Посмотреть состояние подов Deckhouse на кластерах с версией stable (обращаем внимание на `AGE` пода):
  ```bash
  ./***REMOVED*** -s --debug --filter="$(cat /tmp/res | grep :stable | cut -d: -f 1)" --no-prefix ":kubectl: -n d8-system get pod -l app=deckhouse"
  ```

- Посмотреть состояние релизов helm на кластерах на канале `stable`:
  ```bash
  ./***REMOVED*** -s --debug --filter="$(cat /tmp/res | grep :stable | cut -d: -f 1)" --no-prefix ':kubectl: -n d8-system exec -t $(:kubectl: -n d8-system get pod -l app=deckhouse -o name | cut -d/ -f2) -- helm --tiller-namespace=d8-system --host 127.0.0.1:44434 list'
  ```

- Собрать в папку /tmp/logs (должна существовать) логи с кластеров на канале `stable`:
  ```bash
  ./***REMOVED*** -s --debug --filter="$(cat /tmp/res | grep :stable | cut -d: -f 1)" --stdout-dir=/tmp/logs ":kubectl: -n d8-system logs deploy/deckhouse"
  ```

- После того, как собрали логи, можно грепнуть их, например, так (пропуская частые, некритичные сообщения):
  ```bash
  grep -inr error /tmp/logs/ | grep -v 'check image' | grep -v 'get manifest' | grep -v 'too old resource version' | grep -vE 'error copying from local|remote'
  ```
{% endraw %}
{% endofftopic %}

Style Guide
-----------

### Соглашение об именовании

* Для всего, что написано на Shell — мы используем [Shell Style Guide](https://google.github.io/styleguide/shell.xml).
* Для идентификаторов в Kubernetes мы используем [соответствующий стандарт](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/identifiers.md).
* Для Helm Values мы используем camelCase как в Kubernetes, согласно [официальной рекомендации](https://helm.sh/docs/chart_best_practices/values/). Исключение: проброс целиком части values в Kubernetes (например как в случае с nodeSelector).
* Для названий Helm Chart'ов мы используем маленькие буквы и дефисы (kebab-case), согласно [официальной рекомендации](https://helm.sh/docs/chart_best_practices/conventions/).
* Название модуля должно всегда соответствовать названию Helm Chart'а.
* Для названий образов модулей (тех, которые лежат в `modules/*/images/*`) мы используем маленькие буквы и дефисы (чтобы ссылка на image, которая так же содержит имя модуля, не была в разных стилях).
* Переменные в Go-шаблонах в Helm Chat'ах мы именуем camelCase'ом, как это принято в Go.
* Если название нам нужно использовать в разных местах, например в ConfigMap (идентификатор Kubernetes), Helm Values, Shell и Go — мы используем в каждому случае свое соглашение об именовании, соответственно: `use-proxy-protocol`, `useProxyProtocol`, `use_proxy_protocol`. Согласно этому правилу название модуля и название имени образа модуля (которые содержат дефис), когда они используются в Helm Values, становятся camelCase.
* Если нет других причин, в названиях файлов в качестве разделителей мы стараемся использовать подчеркивания `_` и точки `.`, а не дефисы `-`.
* Namespace называем так же как модуль, но с приставкой `d8-` (символизируя этим, что это НЕ пользовательское приложение, а «системный компонент»), например, имя модуля `prometheus`, а имя его namespace — `d8-prometheus`.

### Значения в Helm Values

* Для bool значений используем всегда настоящий bool, а не строку. И используем слова true или false, а не любые другие.
* Для констант используем соглашение, как в Kubernetes — с большой буквы, CamelCase. Например: `LoadBalancer`, `ClusterIP`.

### Обязательные label'ы Deckhouse

У всех ресурсов, которые **создаются и управляются Deckhouse**, должны стоять два label'а:
* `heritage: deckhouse`
* `module: <имя модуля>`

**Внимание!!!** Это не означает, что эти label'ы нужно ставить на объекты, создаваемые другими контроллерами.
Указанные label'ы необходимо ставить только на первичные ресурсы, находящиеся под управлением Deckhouse.

### Рекомендации по использованию label'ов

Рекомендуется использовать label'ы `app` и `component`.


Check-лист для нового модуля
----------------------------
### Bundle
{% raw %}

Bundle - вариант поставки Deckhouse. Варианты:
* `Default` — включает рекомендованный набор модулей для работы кластера: мониторинга, контроля авторизации, организации работы сети и других потребностей. С актуальным списком можно ознакомиться [здесь](https://github.com/deckhouse/deckhouse/tree/master/modules/values-default.yaml).
* `Minimal` — минимально возможная поставка, которая включает единственный модуль `20-deckhouse`.
* `Managed` - поставка для managed кластеров от облачных провайдеров. Список поддерживаемых провайдеров:
   * Google Kubernetes Engine (GKE)

Если ваш модуль должен быть включен по умолчанию в поставку какого-то bundle'а, нужно добавить запись вида `${mobdule_name}Enabled: true` в соответствующий файл `modules/values-${bundle}.yaml`.

Смотри подробнее про алгоритм определения необходимости включения модуля [тут](https://github.com/flant/addon-operator/blob/master/LIFECYCLE.md#modules-discovery).

### Helm

* `helm upgrade --install` вызывается при наличии файла `/modules/<module-name>/Chart.yaml`.
* Для каждого модуля создается отдельный helm-релиз. За создание ресурсов в кластере отвечает Tiller, запущенный отдельным процессом в pod'е Deckhouse.
Просмотр helm-релизов:
  ```bash
  kubectl -n d8-system exec deploy/deckhouse -- helm list
  ```
* При первом выкате helm-релиза, если в кластере уже есть ресурсы, описанные в релизе – выкат в helm упадет. При этом будет создан релиз в состоянии FAILED.
Ошибка будет продолжать появляться пока из кластера не будут удалены повторяющиеся ресурсы.

Контрольная сумма релиза — это контрольная сумма всех файлов helm chart-а и values, которые генерируются в Deckhouse для релиза.

Релизы в helm не обновляются при повторном запуске модуля при выполнении условий:
  * Статус предыдущего релиза не FAILED (можно увидеть в helm list);
  * Контрольная сумма релиза не поменялась.
  * Контрольная сумма всех манифестов в релизе после render'а осталась прежней.

Поэтому повторный запуск модулей не приводит к накоплению холостых ревизий данного helm-релиза.

#### Values для модулей

Values для конкретного модуля объявляются в глобальном ключе с именем модуля. Подробнее про values для модулей читай [тут](https://github.com/flant/addon-operator/blob/master/VALUES.md).

#### Priority Class
Для указания опции `priorityClassName` для pod'ов в `helm_lib` реализован специальный helper.
Необходимо **ОБЯЗАТЕЛЬНО** использовать его во всех контроллерах без исключения.

Пример:
```gotemplate
spec:
{{- include "helm_lib_priority_class" (tuple . "cluster-critical") | indent 2 }}
```
Helper'у на вход передается глобальный context и желаемое значение для priorityClassName. Если в Deckhouse включен модуль `010-priority-class`, тогда шаблон примет вид:
```yaml
spec:
  priorityClassName: cluster-critical
```
Иначе:
```yaml
spec:
```
{% endraw %}

Подробнее о том, какие классы использует Deckhouse можно прочитать в [описании модуля priority-class](/modules/010-priority-class/).

#### Node Selector

Для указания опции `nodeSelector` в `helm_lib` так же реализован специальный helper.

{% raw %}
Пример:
```gotemplate
{{- include "helm_lib_node_selector" (tuple . "monitoring") | indent 6 }}
```
Helper'у на вход передается глобальный context и желаемая стратегия, по которой будет выставлена опции nodeSelector.

Всего существуют четыре стратегии:
1. `frontend`, `system` - работают по следующей логике:
    * Если у модуля в values есть переменная `nodeSelector` - будет использовано её значение. Иначе...
    * Если в кластере были найдены ноды с label'ом `node-role.deckhouse.io/{{ .Chart.Name }}=""`, то в качестве nodeSelector будет использовано это значение. Считается, что это выделенные ноды для компонентов данного chart'а. Иначе...
    * Если в кластере были найдены ноды с лейблом `node-role.deckhouse.io/{{ имя_стратегии }}=""`, то это значение будет использовано в качестве nodeSelector. Считается, что это выделенные ноды для всех компонентов, которые придерживаются данной стратегии деплоя.

2. `monitoring` - работает по той же логике, что `system` и `frontend`, только с дополнительным шагом после всех вышеперечисленных:
    * Если в кластере были найдены ноды с лейблом `node-role.deckhouse.io/system=""`, то в качестве nodeSelector будет использовано это значение. Считается, что если выделенных для мониторинга нод нет, то компоненты подобных модулей должны размещаться на системных нодах.

3. `master` - работает по следующей логике:
    * Если в кластере были найдены ноды с лейблом `node-role.kubernetes.io/master="""`, то это значение будет использовано в качестве nodeSelector. Считается, что это выделенные ноды для всех компонентов, которые придерживаются данной стратегии деплоя.
    * Если в кластере были найдены ноды с лейблом `node-role.deckhouse.io/master="""`, то это значение будет использовано в качестве nodeSelector. то в качестве nodeSelector будет использовано это значение. Считается, что если в кластере нет мастер нод (например, это managed кластер), то компоненты подобных модулей должны размещаться на нодах, указанных в качестве мастеров.
    * Если в кластере были найдены ноды с лейблом `node-role.deckhouse.io/system=""`, то в качестве nodeSelector будет использовано это значение. Считается, что если в кластере нет мастер нод и нод с лейблами, обозначающих ноду как мастер, то компоненты подобных модулей должны размещаться на системных нодах.

Если ни одно из вышеперечисленных для стратегии условий не будет выполнено, nodeSelector проставлен не будет.

Helper **ОБЯЗАТЕЛЬНО** должен быть использован для всех компонентов Deckhouse, в которых это возможно, кроме DaemonSet'ов, всегда выкатываемых на все ноды кластера (node-exporter, csi-node, flannel, etc).

### Tolerations

Для указания опции `tolerations` в `helm_lib` так же реализован специальный helper.

Пример:
```gotemplate
{{- include "helm_lib_tolerations" (tuple . "monitoring") | indent 2 }}
```
Helper'у на вход передается глобальный context и желаемая стратегия, по которой будет выставлена опции tolerations.

* Если у модуля в values есть переменная `tolerations` - будет использовано её значение.
* Если указана стратегия `frontend` или `system`, в манифест будут добавлены следующие правила:

  ```yaml
  tolerations:
  - key: dedicated.deckhouse.io
    operator: Equal
    value: {{ .Chart.Name }}
  - key: dedicated.deckhouse.io
    operator: Equal
    value: {{ имя_стратегии }}
  ```
* Если указана стратегия `monitoring`, правила будут выглядеть так:

  ```yaml
  tolerations:
  - key: dedicated.deckhouse.io
    operator: Equal
    value: {{ .Chart.Name }}
  - key: dedicated.deckhouse.io
    operator: Equal
    value: {{ имя_стратегии }}
  - key: dedicated.deckhouse.io
    operator: Equal
    value: "system"
  ```
* Если указана стратегия `any-node`, правила будут выглядеть так:

  ```yaml
  tolerations:
  - key: node-role.kubernetes.io/master
  - key: dedicated.deckhouse.io
  - key: dedicated
  - key: node.deckhouse.io/uninitialized
    operator: "Exists"
    effect: "NoSchedule"
  - key: node.deckhouse.io/csi-not-bootstrapped
    operator: "Exists"
    effect: "NoSchedule"
  - key: node.kubernetes.io/not-ready
  - key: node.kubernetes.io/out-of-disk
  - key: node.kubernetes.io/memory-pressure
  - key: node.kubernetes.io/disk-pressure
  ```

* Если указана стратегия `wildcard`, правила будут выглядеть так:
  ```yaml
  tolerations:
  - operator: Exists
  ```

Helper **ОБЯЗАТЕЛЬНО** должен быть использован для всех компонентов Deckhouse, в которых это возможно, кроме DaemonSet'ов, всегда выкатываемых на все ноды кластера (node-exporter, csi-node, flannel, etc).


#### Режим HA для модуля

Режим HA или high availability необходим для защиты важных модулей от возможного простоя или отказа в их работе.

Для работы с HA в `helm_lib` существуют вспомогательные темплейты.
* `helm_lib_ha_enabled` - возвращает не пустую строку, если режим HA включен для кластера.
  ```yaml
  {{- if (include "helm_lib_ha_enabled" .) }}
  HA enabled in Kubernetes cluster!
  {{-end }}
  ```
* `helm_lib_is_ha_to_value` - используется в качестве конструкции `if else`. Если в кластере включен режим HA, template вернет первый переданный ему аргумент, если нет — второй.
  ```yaml
  # Если HA включен в кластере, то реплик будет две. Если нет, то одна.
  replicas: {{ include "helm_lib_is_ha_to_value" (list . 2 1) }}
  ```

Чтобы компоненты модуля (Deployment или StatefulSet) корректно работали и обновлялись в режиме HA, существуют правила:

* Обязательно указываем podAntiAffinity для Deployment и StatefulSet, чтобы поды не располагались на одной ноде. Пример для prometheus:

  ```yaml
  {{- include "helm_lib_pod_anti_affinity_for_ha" (list . (dict "app" "deployment-label")) | indent 6 }}
  ```
* Для Deployment правильно выставляем `replicas` и `strategy`:
  * Deployment располагается НЕ на мастер нодах:

    ```yaml
    {{- include "helm_lib_deployment_strategy_and_replicas_for_ha" . | indent 2 }}
    ```
    Это защищает нас от блокировки обновления в случае, когда количество подов Deployment равно количеству нод, и указаны nodeSelector и podAntiAffinity.
  * Deployment располагается на мастер нодах (на каждой!):

    ```yaml
    {{- include "helm_lib_deployment_on_master_strategy_and_replicas_for_ha" . | indent 2 }}
    ```
    Это позволяет нам не блокировать обновление Deployment'а даже если один из мастеров не доступен (при 3 и более узлах)!

### Проверка сложных условий

Если нужно несколько раз проверять сложное условие, рекомендуется самостоятельно реализовать helper (если подобный helper еще не был реализован до вас).

* Если результат выполнения helper'а равен `true`, helper должен вернуть строку вида `not empty string`.
* Если результат выполнения helper'а равен `false`, helper должен вернуть пустую строку.

Пример реализации:
```gotemplate
{{- define "helm_lib_module_https_ingress_tls_enabled" -}}
  {{- $context := . -}}

  {{- $mode := include "helm_lib_module_https_mode" $context -}}

  {{- if or (eq "CertManager" $mode) (eq "CustomCertificate" $mode) -}}
    not empty string
  {{- end -}}
{{- end -}}
```
Пример использования:
```gotemplate
{{- if (include "helm_lib_module_https_ingress_tls_enabled" .) }}
- name: ca-certificates
  mountPath: "/usr/local/share/ca-certificates/"
  readOnly: true
{{- end }}
```

### Hooks

Подробнее о хуках, их устройстве и привязки к событиям смотри в документации addon-operator [тут](https://github.com/flant/addon-operator/blob/master/HOOKS.md).

В Deckhouse **глобальные хуки** располагаются в директории `/global-hooks`, **хуки модулей** располагаются в директории `/modules/MODULE/hooks` модуля.

Чтобы передать информацию в хук используются переменные среды с путями к файлам в /tmp. Хук возвращает результат тоже через файлы. Подробнее про использование параметров в хуках читай [тут](https://github.com/flant/addon-operator/blob/master/VALUES.md).

### Validating admission webhooks

По интерфейсам и механизму работы хуки валидации похожи на обычные хуки. Для них доступен тот же shell-фреймворк. Подробнее о хуках для валидации см. [документацию shell-operator](https://github.com/flant/shell-operator/blob/feat_validating_webhook/BINDING_VALIDATING.md).

В Deckhouse хуки валидации располагаются в директории `/modules/MODULE/webhooks/validation/` модуля.

### Conversion webhooks

По интерфейсам и механизму работы хуки конвертации похожи на обычные хуки. Для них доступен тот же shell-фреймворк. Подробнее о хуках для валидации см. [документацию shell-operator](https://github.com/flant/shell-operator/blob/feat_conversion_binding/BINDING_CONVERSION.md).

В Deckhouse хуки валидации располагаются в директории `/modules/MODULE/webhooks/conversion/` модуля.

#### kubectl

В хуках не рекомендуется использовать kubectl. Это приводит к потере идемпотентности, т.к. помимо зависимости от входных параметров хук начинает быть зависимым еще и от состояния кластера (что создает определенные проблемы при отладке и тестировании).
* Для слежения за объектами необходимо пользоваться [встроенным функционалом shell-operator](https://github.com/flant/shell-operator/blob/master/HOOKS.md#kubernetes), который польностью интегрирован в Deckhouse.
* Для создания, изменения и удаления объектов необходимо использовать функционал shell_lib, в частности функции с префиксом `kubernetes::` (kubernetes::create_yaml, kubernetes::patch_jq, kubernetes::delete_if_exists, etc).

#### enabled-хуки

enabled-хуки располагаются в корневой директории модуля. С их помощью можно описать условия, при которых модуль должен обязательно быть включен или выключен.

Пример:
```bash
#!/bin/bash

source /deckhouse/shell_lib.sh

function __main__() {
  if values::has global.modules.publicDomainTemplate ; then
    echo "true" > $MODULE_ENABLED_RESULT
  else
    echo "false" > $MODULE_ENABLED_RESULT
  fi
}

enabled::run $@
 ```
Данный хук выключит модуль во всех кластерах, где не выставлена опция `global.modules.publicDomainTemplate`.

Частые проверки реализованы в функциях в `shell_lib` с префиксом `enabled::`. Например
```bash
function __main__() {
  enabled::disable_module_in_kubernetes_versions_less_than 1.15.0
  echo "true" > $MODULE_ENABLED_RESULT
}
```
выключит модуль во всех кластер с версией Kubernetes ниже 1.15.0.

Дополнительную информацию можно [прочитать в документации](https://github.com/flant/addon-operator/blob/master/LIFECYCLE.md#enabled-script).

### OpenAPI схемы для валидации значений

Deckhouse поддерживает валидацию для значений, передаваемых пользователем через ConfigMap Deckhouse и для значений, формируемых самим Deckhouse.

Валидация значений необходима для того, чтобы быть уверенными, что:
  * Пользователь написал в ConfigMap Deckhouse только те значения, которые мы считаем допустимыми. И в случае, если пользователь написал неверные значения, чтобы он об этом узнал.
  * Для рендера helm-шаблонов модуля переданы все необходимые параметры в нужном формате, чтобы поведение было ожидаемо и в кластере оказались точно те объекты, которые мы хотим там видеть.

Схемы валидации OpenAPI хранятся в директории `$GLOBAL_HOOKS_DIR/openapi` для глобальных значений и
в `$MODULES_DIR/<module-name>/openapi` - для модулей.

Подробно о валидации схем можно прочесть в [документации к addon-operator](https://github.com/flant/addon-operator/blob/master/VALUES.md#validation).

Формат схем валидации - OpenAPI Schema Object. Подробнее формат описан в [документации](http://json-schema.org/understanding-json-schema/).

Формат схем расширен дополнительными свойствами в `addon-operator`. Подробнее - в [документации](https://github.com/flant/addon-operator/blob/master/VALUES.md#extending).

**Внимание!!!** Если свойство `additionalProperties` не определено, то по умолчанию оно примет значение `false` на всех уровнях схемы !!!

* Схема `openapi/config-values.yaml` предназначена для валидации значений, передаваемых пользователем через ConfigMap.

Пример:
```
type: object
properties:
  podNetworkMode:
    type: string
    enum: ["host-gw", "vxlan"]
    default: "host-gw"
    description: |
      Режим работы `host-gw` или `vxlan`.
```
* Схема `openapi/values.yaml` предназначена для валидации объединенных значений, состоящих из значений из ConfigMap и значений, сформированных хуками (подробнее [тут](https://github.com/flant/addon-operator/blob/master/VALUES.md#merged-values)).

  * **Важно !!!** Сама схема `openapi/values.yaml` предназначена для валидации значений, сформированных хуками. Поэтому при валидации объединенных значений схема будет давать ошибку, поскольку в ней не описаны значения, получаемые из ConfigMap.
    Чтобы не дублировать параметры из схемы `openapi/config-values.yaml`, предусмотрен специальный параметр `x-extend`, расширяющий схему `openapi/values.yaml` параметрами схемы `openapi/config-values.yaml`, как в примере ниже.
    Параметр `x-extend` необходимо использовать всегда. Подробнее [тут](https://github.com/flant/addon-operator/blob/master/VALUES.md#extending).

Пример:
```
x-extend:
  schema: config-values.yaml
type: object
properties:
  internal:
    type: object
    default: {}
    x-required-for-helm:
    - podNetworkMode
    properties:
      podNetworkMode:
        type: string
        enum: ["host-gw", "vxlan"]
```

Порядок создания схемы валидации для модуля:
* `openapi/config-values.yaml`:
  * Cхема формируется на основании документации модуля.
  * Проставить дефолтные значения для полей. Дефолтные значения могут быть:
    * В документации.
    * В файле `$MODULES_DIR/<module-name>/values.yaml`.
    * Захардкожены в тексте хуков.
  * Для всех обязательных полей должно быть проставлено свойство `required`.
* `openapi/values.yaml`:
  * Схема формируется для значений, устанавливаемых хуками (обычно, это переменные в группе `internal`).
  * Проставить свойство `x-extend` для загрузки схемы `config-values.yaml`.
  * Проставить дефолтные значения для полей. Дефолтные значения могут быть:
    * В файле `$MODULES_DIR/<module-name>/values.yaml`.
    * Захардкожены в тексте хуков.
  * Для всех обязательных полей должно быть проставлено свойство `x-required-for-helm`.

После создания схем для модуля необходимо удалить файл `$MODULES_DIR/<module-name>/values.yaml`.

### Использование storage class

В случае, если модуль использует Persistent Storage, то необходимо вычислять effective storage class следующим образом:
1. Если указан в values (в конфиге) модуля – использовать явно указанный в модуле.
2. Если уже существует PV – использовать storage class из существующей PV.
3. Иначе использовать или определенный глобально или дефолтный, вычисленный автоматически.
4. Если ничего из этого, то использовать emptyDir.

При таком подходе, при изменении глобального storage class или storage class по умолчанию, перезаказ PV не происходит и данные не удаляются. Чтобы перезаказать PV нужно явно, прямо в конфигурации модуля, указать другой storage class.

Нужно не забыть, что `volumeClaimTemplate` мутировать нельзя.  Поэтому при смене storageClass нужно не забыть удалить statefulset (например использовать для этого хук).

Пример можно смотреть в хуках в модулях [prometheus](https://github.com/deckhouse/deckhouse/tree/master/modules/300-prometheus/hooks/prometheus_storage_class_change) и [openvpn](https://github.com/deckhouse/deckhouse/tree/master/modules/500-openvpn/hooks/storage_class_change).

### CRD

Должны находиться в папке `crds` в корне модуля.

Для применения в модуле должен быть специальный хук с названием `ensure_crds` и содержимым:
```bash
#!/bin/bash

source /deckhouse/shell_lib.sh

function __config__() {
  common_hooks::https::ensure_crds::config
}

function __main__() {
  common_hooks::https::ensure_crds::main $(module::path)/crds/*
}

hook::run "$@"
```

Если ресурсы, описанные при помощи CRD, используются в других модулях - необходимо вынести CRD в отдельный модуль.

Пример - `010-vertical-pod-autoscaled-crd`. Нужны нам практически в каждом модуле.

#### Разработка CRD

1. Схема для проверки ресурса `openAPIV3Schema` должна быть описана максимально подробно, `description` для объектов должны быть написаны на английском языке.
2. Добавляйте описание дополнительных колонок через `spec.additionalPrinterColumns`. Они будут отображаться при запросах через `kubectl get` и помогут улучшить взаимодействие с пользователем.

### Тестирование

Каждый модуль должен быть покрыт тестами. Существует три вида тестов:
* Тесты хуков - располагаются в папке hooks, именуются как `${имя_хука}_test.go`. Проверяют результат выполнения хуков.
* Тесты хельма - располагаются в отдельной папке `template_tests` в корне модуля. Проверяют логику в шаблонах хельма.
* Матричные тесты - описываются в виде файла `values_matrix_test.yaml` в корневой директории модуля. Проверяют рендер шаблонов хельма и соответствие этих шаблонов нашим стандартам на большом количестве values.yaml, описанных матрицей.

Поиск проблем в работе Deckhouse
--------------------------------

### Debug
В addon-operator существуют специальные команды, призванные упростить поиск проблем, связанных с работой Deckhouse.

Подробнее о них можно узнать, выполнив команду:
```bash
kubectl -n d8-system exec deploy/deckhouse -- deckhouse-controller help
```
либо прочитав [документацию](https://github.com/flant/addon-operator/blob/master/RUNNING.md#debug).

### Скрипт для получения всей необходимой debug-информации

Необходимо на мастере выполнить скрипт:
```shell
#!/bin/bash

# Prepare deckhouse info for debug
deckhouse_pod=$(kubectl -n d8-system  get pod -l app=deckhouse -o name)
deckhouse_address=$(kubectl -n d8-system  get pod -l app=deckhouse -o json | jq '.items[] | .status.podIP' -r)
deckhouse_debug_dir=$(mktemp -d)
debug_date=$(date +%s)

# Get deckhouse version
kubectl -n d8-system exec -ti ${deckhouse_pod} -- deckhouse-controller version > ${deckhouse_debug_dir}/version
# Get go trace
curl -s ${deckhouse_address}:9650/debug/pprof/trace?seconds=60 > ${deckhouse_debug_dir}/trace
# Get goroutine
curl -s ${deckhouse_address}:9650/debug/pprof/goroutine > ${deckhouse_debug_dir}/goroutine
# Get go heap
curl -s ${deckhouse_address}:9650/debug/pprof/heap > ${deckhouse_debug_dir}/heap
# Get process dump
curl -s ${deckhouse_address}:9650/debug/pprof/profile?seconds=60 > ${deckhouse_debug_dir}/profile
# Get process list
kubectl -n d8-system  exec -ti $deckhouse_pod -- ps auxfww > ${deckhouse_debug_dir}/ps_aux
# Get deckhouse log
kubectl -n d8-system  logs $deckhouse_pod  > ${deckhouse_debug_dir}/log
# Get deckhouse metrics
curl -s ${deckhouse_address}:9650/metrics > ${deckhouse_debug_dir}/metrics
# Get deckhouse queue
kubectl -n d8-system exec -ti ${deckhouse_pod} -- deckhouse-controller queue list > ${deckhouse_debug_dir}/queue_list
# Get modules values
mkdir ${deckhouse_debug_dir}/values
for module in $(kubectl -n d8-system exec -ti ${deckhouse_pod} -- helm list | grep -v NAME | awk '{print $1}'); do kubectl -n d8-system exec -ti ${deckhouse_pod} -- deckhouse-controller module values ${module} -o json > ${deckhouse_debug_dir}/values/${module}; done

# tar debug files
tar -czf /tmp/deckhouse_debug_${debug_date}.tar.gz ${deckhouse_debug_dir}
ls -lah /tmp/deckhouse_debug_${debug_date}.tar.gz

# Clear debug folder
rm -rf ${deckhouse_debug_dir}
```
{% endraw %}

Данный скрипт выполняется ~2.5 минуты. На выходе будет `.tar.gz` файл, который необходимо передать разработчикам deckhouse.

### Метрики для prometheus

Описание и список экспортируемых метрик смотри [тут](https://github.com/flant/addon-operator/blob/master/METRICS.md).

### Просмотр логов Deckhouse

На текущий момент все логи Deckhouse выводятся в формате json. Для приведения их к удобному для просмотра
виду предлагается использовать `jq`, который прекрасно умеет преобразовывать строки из потока.

#### Примеры:
{% offtopic title="Вывод логов для каждого модуля" %}
{% raw %}

* С цветом:
```bash
kubectl -n d8-system logs deploy/deckhouse -f | jq -r 'select(.module != null) | .color |= (if .level == "error" then 1 else 4 end) | "\(.time) \u001B[1;3\(.color)m[\(.level)]\u001B[0m\u001B[1;35m[\(.module)]\u001B[0m - \u001B[1;33m\(.msg)\u001B[0m"'
```
* Монохромный вариант:
```bash
kubectl -n d8-system logs deploy/deckhouse -f | jq -r 'select(.module != null) | "\(.time) [\(.level)][\(.module)] - \(.msg)"'
```
* Конкретный модуль:
```bash
kubectl -n d8-system logs deploy/deckhouse -f | jq -r --arg mod cloud-instance-manager 'select(.module == $mod) | "\(.time) [\(.level)][\(.module)] - \(.binding) - \(.msg)"'
```
{% endraw %}
{% endofftopic %}

{% offtopic title="Вывод логов для каждого хука" %}
{% raw %}

* С цветом:
```bash
kubectl -n d8-system logs deploy/deckhouse -f | jq -r 'select(.hook != null) | .color |= (if .level == "error" then 1 else 4 end) | "\(.time) \u001B[1;3\(.color)m[\(.level)]\u001B[0m\u001B[1;35m[\(.hook)]\u001B[0m - \(.binding) - \u001B[1;33m\(.msg)\u001B[0m"'
```
* Монохромный вариант:
```bash
kubectl -n d8-system logs deploy/deckhouse -f | jq -r 'select(.hook != null) | "\(.time) [\(.level)][\(.hook)] - \(.binding) - \(.msg)"'
```
* Конкретный хук:
```bash
kubectl -n d8-system logs deploy/deckhouse -f | jq -r --arg hook 402-ingress-nginx/hooks/ensure_crds 'select(.hook == $hook) | "\(.time) [\(.level)][\(.hook)] - \(.binding) - \(.msg)"'
```
{% endraw %}
{% endofftopic %}

Отладка хуков
-------------
{% raw %}

* В любом месте любого хука можно написать `debug::breakpoint 127.0.0.1 4284`. Хук при этом зависнет и будет ожидать подключения на указанный порт.
* Подключиться можно самым обычным telnet'ом (`telnet 127.0.0.1 4284`). Любая введенная команда будет eval'нута в контексте в котором был вызван `debug::breakpoint`, вы получите ее output.
* Лучше всего начинать сессию откладки с установки `set +e`, чтобы хук не вышел при первой же ошибке.
* Если отладку необходимо запустить только в определенной ситуации — `debug::breakpoint` нужно поставить под if.
* При локальной разработке рекомендуется использовать адрес `0.0.0.0` и порт `4284`, тогда можно делать telnet прямо с локальной машины, минуя вход в контейнер.

Локальная разработка
--------------------
Для удобной разработки необходимо добавить следующий symlink:
```bash
sudo ln -s $(pwd) /deckhouse
```
В MacOS, если корневая система в режиме Read-only, можно так же добавить подобный symlink командой:
```bash
echo "deckhouse\t$(pwd)" >> /etc/synthetic.conf
```
c последующей перезагрузкой системы.

NB: Обе команды должны быть выполнены в корневой директории данного репозитория.

### Запуск тестов

1. [Залогиниться](https://pult.flant.com/projects/dev-rnd/services/0d5280b0-9331-4cc2-ab2d-b2761b711324) в docker registry для хранения werf-стейджей:

```
docker login https://registry-stages.flant.com:5000/
```

2. Выполнить скрипт `./testing/run`, передав ему название файла.

Тип                           | Файл
------------------------------|-----------------------------------------------------------------
Все тесты в текущей папке     | `./testing/run .`
Все матричные тесты           | `./testing/run testing/matrix`
Тесты конфигураций хуков      | `./testing/run testing/hooks_configuration`
Все тесты глобальных хуков    | `./testing/run global-hooks`
Все тесты одного модуля       | `./testing/run modules/150-user-authn`
Helm-тесты модуля             | `./testing/run modules/150-user-authn/template_tests`<small><br />(любой файл в директории template_tests или templates)</small>
Матричные тесты модуля        | `./testing/run modules/150-user-authn/values_matrix_test.yaml`
Все тесты хуков одного модуля | `./testing/run modules/150-user-authn/hooks`
Тест одного хука              | `./testing/run modules/150-user-authn/hooks/some_hook_test.go`<small><br />или<br /></small>`./testing/run ./modules/150-user-authn/hooks/some_hook`<small><br />или<br /></small>`./testing/run ./gobal-hooks/some_hook_test.go`<small><br />или<br /></small>`./testing/run ./global-hooks/some_hook`<small><br />(как запускать отдельные `Context` или отдельные `It` можно узнать, прочитав [документацию Ginkgo](https://onsi.github.io/ginkgo/#focused-specs))</small>
Все тесты хуков из директории | `./testing/run modules/150-user-authn/hooks/some_dir/and_yet_another`<small><br />или<br /></small>`./testing/run ./global-hooks/some_dir/and_yet_another`

**Важно!** Для запуска тестов нужен multiwerf, необходимо [его установить](https://werf.io/documentation/guides/installation.html).

Для входа в контейнер, в котором запускаются тесты:
```
docker exec -ti deckhouse-testing bash
```
{% endraw %}
### Запуск и отладка тестов из Goland

#### Использование

#### Запуск тестов

1. Выбираем необходимую Run/Debug Configuration в правом верхнем углу экрана:

    ![](./images/running-tests-from-golang/00-select-configuration.jpg)
2. Выбираем файл (или директорию), для которой запустить тесты (или в Project, или просто держим фокус на открытом файле).
3. Запускаем тесты, нажав `Ctrl-R`.

**Важно!**
1. Первый запуск занимает некоторое время, не паниковать.
2. Кнопка остановки работает корректно – если надо остановить тест, достаточно просто на нее нажать (не работает в отладке).
#### Отладка тестов

1. Ставим brake point.
2. Меняем Run/Debug Configuration на Debug Tests.
3. Удостоверяемся, что запускаемый файл находится в фокусе.
4. Запускаем отладку тестов, нажав `Ctrl-D`

**Важно!**
1. Первый запуск занимает некоторое время, не паниковать.
1. Из-за багов в Goland в настоящий момент нет возможности посмотреть stdout во время debug'а, поэтому для отладки можно запустить в терминале следующую команду:
    ```bash
    tail -f /tmp/deckhouse-testing-debug.log
    ```
2. Остановка отладки не всегда работает через кнопку Stop – если вы случайно запустили что-то, что будет очень долго выполняться и грузить проц (например все тесты), то для убийства надо зайти в контейнер deckhouse-testing и прибить процессы:
3. Если по какой-то причине hotkey перестал работать (и пропала кнопка Debug у конфигурации Debug Tests) – нужно найти в системе запущенный скрипт testing/run и прибить его.

#### Настройка

1. Добавить два External Tool:
    1. Открыть раздел External Tools в Preference'ах проекта и нажать добавить:

        ![](./images/running-tests-from-golang/01-external-tools.jpg)
    2. Ввести следующие параметры и создать external tool для запуска тестов:

        ![](./images/running-tests-from-golang/02-external-tool-for-running-tests.jpg)
    3. Аналогичным оразом создать второй external tool, для отладки тестов

        ![](./images/running-tests-from-golang/03-external-tool-for-debugging-tests.jpg)

        **Важно!!!** Добавлен параметр `--debug`, все checkbox'ы сняты!
2. Добавить Run/Debug Configuration **для запуска тестов**:
    1. Открыть в верхнем меню Run -> Edit Configurations
    2. Добавить конфигурацию на основе Shell

        ![](./images/running-tests-from-golang/04-create-run-configuration.jpg)
    3. Заполнить следующим образом:

        ![](./images/running-tests-from-golang/05-setup-run-configuration.jpg)
3. Добавить Run/Debug Configuration **для отладки тестов**:
    1. Открыть в верхнем меню Run -> Edit Configurations
    2. Добавить конфигурацию на основе Go Remote

        ![](./images/running-tests-from-golang/06-create-debug-configuration.jpg)
    3. Заполнить следующим образом:

        ![](./images/running-tests-from-golang/07-setup-debug-configuration.jpg)

Минорное обновление версий kubernetes, поддерживаемых Deckhouse
------------------------------------------------------

При выходе новых патч-версий [Kubernetes](https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG), необходимо обновить patch-версии до актуальных в файле [version_map.yml](../../candi/version_map.yml).
