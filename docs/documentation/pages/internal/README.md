## Что делают хуки и какие они бывают?

В основном, хуки делают одно или несколько из следующих действий:
 * выполняют некоторый *"discovery"* и генерируют Helm Values (или глобальные или для своего модуля), которые затем используются в Helm Chart'ах
 * вносят изменения в конфиг deckhouse (например, генерируют пароль, если его нет)
 * удаляют объекты (например, удаляют конфликтующие объекты перед установкой Helm Chart'а модуля)
 * некоторые глобальные хуки вносят изменения в те объекты, которые не находятся под управлением helm

Глобальные хуки в *Deckhouse* лежат в директории `global-hooks/*`, хуки модулей — в `modules/*/hooks/*`. Существует возможность привязать запуск хука к одному или нескольким [событиям](https://github.com/flant/addon-operator/blob/master/HOOKS.md#overview).

Для того чтобы посмотреть, когда будет запускаться конкретный хук — хук можно вызвать с параметром `--config`, при этом хук должен вернуть JSON в "интуитивно-понятном" формате. Подробнее про формат настройки смотри [тут](https://github.com/flant/addon-operator/blob/master/HOOKS.md#bindings)

Кроме обычных хуков у модуля может быть специальный детектор включенности (располагается в `modules/*/enabled`) — он выполняется после отработки глобальных хуков, но до запуска всех модулей. Подробнее про жизненный цикл модуля смотри [тут](https://github.com/flant/addon-operator/blob/master/LIFECYCLE.md).

## Разворачивание и управление узлами в кластерах Kubernetes
Для создания новых кластеров и управления узлами используется подсистема [candi (Cluster and Infrastructure)](/candi/). Ключевые преимущества:
* Единый процесс установки и управления кластерами Kubernetes для baremetal и cloud инсталляций
* Простое управляемое обновление:
   * компонентов kubernetes (control-plane, kubelet)
   * компонентов системы (ядро, докер, прочие пакеты)
* Декларативный стиль описания всех компонентов инфраструктуры кластера в процессе установки и использования

**Candi включает в себя**:
* Ядро общего функционала, используемое в модулях и инсталляторе:
    * Набор простых идемпотентных скриптов на bash, которые конфигурируют узлы (см. подробнее [bashible](/candi/bashible/)).
    * Шаблоны конфигурации kubeadm (kubeadm-config.yaml и патчи), которыми конфигурируется control-plane.
    * Для каждого поддерживаемого облачного провайдера – необходимые тераформы и дополнительные скрипты конфигурации узлов.

* Инсталлятор кластера и deckhouse:
    * В облаках installer использует terraform для создания инфраструктуры и отдельный terraform для создания первого узла (при установке происходит два запуска!)
    * State terraform'а, оставшийся после создания базовой инфраструктуры, сохраняется в кластер в namespace `kube-system` в secret `d8-terraform-state`.

* Модуль [control-plane-manager](/modules/040-control-plane-manager/) — реализация `managed` control plane:
    * При использовании этого модуля обновление и настройка компонентов control plane полностью переходят под управление Deckhouse. 
    * Обновление patch-версии будет происходить автоматически при релизах Deckhouse. Точность версии в конфигурации можно указать только до minor-версии (например `1.16`).
        * В Deckhouse для каждой поддерживаемой минорной версии определена **точная версия**. Версия кластера `1.15`, точная версия `1.15.9`.
        * Точная версия может не совпадать с максимально доступной версией в репозитории kubernetes.
    * Работает как для singlemaster, так и для multimaster кластеров. Позволяет добавлять в кластер новые master-узлы и удалять старые.

* Модуль [node-manager](/modules/040-node-manager/) — реализации `managed` узлов (нод).
    * Работает как в облаке, так и в baremetal кластерах.
    * Поддерживаемые типы узлов: Static, Hybrid, Cloud - подробнее о каждом типе написано в документации модуля.
    * Умно и безопасно по одному обновляет (или перекатывает) узлы при изменении настроек (например версии докера, ядра).
    * Позволяет пользователю отключить автоматическое обновление узлов и самостоятельно контролировать процесс, оповещает о необходимом обновления при помощи алертов.
    * Для управления узлами в кластере используется специальный ресурс - `NodeGroup`.
    * Настройка узла и управление им реализованы при помощи [bashible](/candi/bashible/).

* Модули Deckhouse `cloud-provider-` для взаимодействия с облачной инфраструктурой:
    * Провайдеры, которые полностью поддерживают candi:

        | Провайдер     | Варианты установки |
        | ------------- | ------------------ |
        | [cloud-provider-openstack](/modules/030-cloud-provider-openstack/)  | [layouts](/candi/cloud-providers/openstack/) |
    
    * Необходимую информацию для подключения к API и настройки cloud-provider'ы берут из secret'ов в namespace `kube-system`, либо из настроек модуля.


### Разворачивание кластера и установка Deckhouse:

Разворачивание кластера производится при помощи [специального приложения](/candi/dhctl.html) `dhctl` (или installer).
Installer принимает на вход единственный YAML-файл, в котором описана конфигурация для развертывания кластера.

{% raw %}
Сокращенный пример файла конфигурации:
```yaml
apiVersion: deckhouse.io/v1
kind: ClusterConfiguration
clusterType: Static
podSubnetCIDR: 10.111.0.0/16
serviceSubnetCIDR: 10.222.0.0/16
kubernetesVersion: "1.16"
clusterDomain: "cluster.local"
---
apiVersion: deckhouse.io/v1
kind: InitConfiguration
sshPublicKeys:
- ...
masterNodeGroup:
  ...
deckhouse:
  imagesRepo: registry.deckhouse.io/fe
  registryDockerCfg: ...
  releaseChannel: Alpha
  bundle: Default
  configOverrides:
    global:
      clusterName: main
      project: pivot
```
{% endraw %}
Так же необходимо указать параметры подключения по ssh, чтобы попасть на сервер для подготовки инфраструктуры и установки deckhouse.

Пример команды запуска установки кластера:
```bash
dhctl bootstrap \
  --ssh-user=ubuntu \
  --ssh-agent-private-keys=~/.ssh/tfadm-id-rsa \
  --ssh-bastion-user=y.gagarin \
  --ssh-bastion-host=tf.hf-bastion \
  --config=/config.yaml 
```
Для удобного запуска подготовлен специальный Docker-образ.

Обратите внимание на поле `deckhouse.bundle` в InitConfiguration. Выбранный bundle определяет устанавливаемые по умолчанию модули Deckhouse. Подробнее читайте в [документации модуля deckhouse](/modules/020-deckhouse/).

#### Варианты установки Deckhouse:
Сейчас Deckhouse поддерживает 4 варианта установки:
* **Установка в baremetal-кластера** - dhctl подключается к подготовленному серверу по SSH, устанавливает зависимости, последнее ядро linux, docker и control-plane, после чего устанавливает Deckhouse. 
   * В конфигурации необходимо указать `InitConfiguration` и `ClusterConfiguration` с `clusterType: Static`
   * Выбрать bundle - `Default`

* **Установка в облако** - dhctl при помощи Terraform в облаке создает виртуальную машину, после чего подключается к ней по SSH и выполняет те же действия, что и для baremetal-кластера.
   * В конфигурации необходимо указать `InitConfiguration` и `ClusterConfiguration` с `clusterType: Cloud`
   * Так же конфигурации необходимо указать секции, специфичные для вашего облачного провайдера (дл OpenStack это будут `OpenStackInitConfiguration` и `OpenStackClusterConfiguration`)
   * Выбрать bundle - `Default`

* _Coming_Soon_: **Установка в managed-кластера (EKS, GKE и другие)** - TODO
   * В конфигурации необходимо указать - TODO
   * Выбрать bundle - `Managed`

* _Coming_Soon_: **Установка в уже существующий кластер** - dhctl подключается к уже работающему Kubernetes-кластеру и устанавливает Deckhouse. 
   * В конфигурации необходимо указать - TODO
   * Выбрать bundle - `Minimal`
