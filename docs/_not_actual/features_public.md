
Управляет кластером Kubernetes в соответствии с нашими лучшими практиками.

Консолидирует многолетний опыт компании, автоматизирует рутинные операции и реализует функции и настройки, которые так или иначе нужны в каждом кластере.

| возможности | описание | статус |
|------|----------|--------|
| Простота использования | Простая установка, продуманные фичи, автоматизирующие рутину | <a href="#easyuse">включено</a> |
| Мониторинг | Сбор и хранение метрик, предустановленные Dashboard-ы и графики | <a href="#monitoring">включено</a> |
| Улучшенный Ingress | Интеграция с внешними балансировщиками, Расширенный мониторинг трафика и другое | <a href="#balancer">включено</a> |
| Управление ресурсами | Реализует автомасштабирование, как вертикальное, так и горизонтальное - по ресурсам, кастомным или внешним метрикам. Автоматическое перераспределение нагрузки н аноды с помощью Descheduler. Приоритезация Pod-ов и предустановленные PriorityClass. | скоро |
| Управление SSL-сертификатами | Заказывает Let’s Encrypt и Self-signed сертификаты. Автоматически устанавливает и продляет сертификаты для базовых компонентов Deckhouse. | скоро |
| DNS | Ускорение работы приложений за счет дополнительного размещения dns-серверов (CoreDNS) на каждой ноде кластера | скоро |
| Безопасность | Простое управление ролями пользователей; Аутентификация с помощью OpenID-провайдеров; OpenVPN-сервер с панелью управления для защищённого доступа к кластеру | скоро |
| Dashboard | Веб-интерфейс для управления Pod-ами и другими высокоуровневыми ресурсами; Удалённый доступ в контейнеры через веб-консоль для отладки; Просмотр логов отдельных контейнеров | скоро |

<a name="easyuse" />

# Простота использования

Мы стремимся поддерживать максимальную простоту установки и эксплуатации. Это подразумевает:

- Простоту установки - одной командой вы получаете настроенный кластер (см. [инструкцию по установке](/guide/install.html))
- Автоматическое обновление - обновления происходят автоматически, и вы сами указываете - получать ли новые релизы раньше в режиме early access, или получать только стабильные и проверенные временем решения (см. [инструкцию об обновлениях](/guide/update.html))
- Автоматизация типичных рутинных операций, которые так или иначе приходится делать в каждом кластере

## Работа со всеми платформами

Кластер одинаково хорошо поддерживает популярные cloud-провайдеры, onprem-инсталляции и bare metal решения. Благодаря грамотной архитектуре вы не будете завязаны на специфику хостинг-сервисов.

Решение совместимо и протестировано на практике с

- AWS
- Google cloud
- Azure
- vSphere
- OpenStack

Существует только одно ограничение - один кластер может находиться только в одном регионе, на одной хостинг-платформе.

Подробнее о совместимости и ограничениях читайте в [статье о совместимости](/requirements/compatibility.html).

<a name="monitoring" />

# Мониторинг

Deckhouse устанавливает в ваш кластер настроенный Prometheus и dashboard-ы с продуманными графиками.

## Сбор и хранение данных Prometheus-ом

<img src="http://placehold.it/900x450" />

- Метрики могут получаться по pull и push-модели.
- Детализированная статистика (с интервалом сбора в 30 секунд) хранится в течение последних 15 дней
- Агрегированная статистика для отслеживания трендов (с интервалом сбора в 5 минут) хранится в течение нескольких месяцев/лет
- Сроки хранения могут быть переопределены
- Опциональный режим отказоустойчивости и высокой доступности компонентов мониторинга

## Предустановленные Dashboard-ы и графики

В кластер автоматически устанавливается Grafana с предустановленными графиками, отображающими информацию о состоянии кластера и ключевых системных компонентов.

### Общая утилизация кластера

Графики по утилизированным и недоутилизированным серверным мощностям. Показывает общую утилизацию памяти, CPU и количества Pod-ов и позволяет планировать общее количество ресурсов в кластере и затраты на серверное оборудование.

### Состоянию узлов кластера

Графики для отслеживания утилизации узлов позволяют диагностировать проблемы, связанные с загрузкой серверов.

- загрузка процессора (утилизация, Load Average)
- память
- чтение и запись на диск
- общая сетевая активность (приём и отдача)

А также более детально изучать состояние каждого конкретного узла, в частности:

- траффик в разрезе по интерфейсам
- занятое место на дисках
- CPU, потраченный на процессы ядра, пользователя, простой, ожидание i/o и другое
- показатели чтения и записи диска в разрезе по дискам
- ошибки в передаче траффика по сети, в разрезе по интерфейсам

Всего более 150 графиков, помогающих глубоко продиагностировать проблемы с диском, памятью и сетью.

### Сетевой доступности узлов

Отслеживание сетевой доступности узлов между всеми узлами и до дополнительных внешних узлов. Они помогают продиагностировать проблемы со связью между компонентами в кластере и между компонентами и внешними зависимостиями.

- Потери пакетов
- Round Trip Time
- Mean deviation

> Модуль: [ping-exporter]({/docs/modules/ping-exporter/)

<a name="balancer" />

# Улучшенный Ingress

В кластер устанавливается улучшенный Nginx Ingress, адаптированный к популярным cloud-провайдерам и средствам защиты от DDoS. 

Мы переосмыслили и закрыли ими потенциальные ошибки в формировании конфигурации. Мы уже решили проблемы, до которых рядовому пользователю только предстоит дойти.

Ingress интегрируется с Prometheus, показывая множество данных о входящем и исходящем из кластера траффике.

## Адаптированный к популярным cloud-провайдерам

Установка контроллеров для различных типов кластеров может отличаться. Параметров для Ingress в официальном helm-chart очень много, настроить их корректно очень тяжело.

Доработанный нами Nginx Ingress учитывает особенности ключевых клауд провайдеров (Azure, AWS, Google Cloud) и позволяет гибко настраивать работу с сервисами защиты от ddos (QRator, CloudFlare и другие). Deckhouse автоматически настраивает все необходимые параметры и устанавливает в кластер ingress-controller (или несколько). В большинстве проектов конфиг для модуля Ingress будет пустым, а в случае необходимости настроек модуль предоставляет верхнеуровневые понятные настройки. 

## Отказоустойчивая обработка трафика

Для поступающего напрямую трафика в BareMetal кластера мы разработали схему с резервированием - direct. Падения и обновления контроллеров больше НИКОГДА не будут вызывать простой в предоставлении услуг.

Модуль позволяет определить для кластера несколько контроллеров, это позволит разделить приходящий трафик и обрабатывать его разными способами. Делается это простым добавление нескольких строчек в конфигурации.

## Расширенный мониторинг трафика

Наш балансировщик позволяет производить более глубокую диагностику благодаря дополнительным метрикам. Мы написали скрипт на lua, который экспортируют данные в доработанный statsd. Решение проверено и гарантировано не вызывает проблем в production окружениях с большими нагрузками. Собирается много метрик:

* У всех собираемых метрик есть служебные лейблы, позволяющие идентифицировать экземпляр контроллера: `controller`, `app`, `instance` и `endpoint`.
* Все метрики (кроме geo), экспортируемые statsd_exporter'ом, представлены в трех уровнях детализации:
    * `ingress_nginx_overall_*` — "вид с вертолета", у всех метрик есть лейблы `namespace`, `vhost` и `content_kind`.
    * `ingress_nginx_detail_*` — кроме лейблов уровня overall добавляются: `ingress`, `service`, `service_port` и `location`.
    * `ingress_nginx_detail_backend_*` — ограниченная часть данных, собирается в разрезе по бекендам. У этих метрик, кроме лейблов уровня detail, добавляестя лейбл `pod_ip`.
* Для уровней overall и detail собираются следующие метрики:
    * `..._requests_total` — counter количества запросов (дополнительные лейблы: `scheme`, `method`).
    * `..._responses_total` — counter количества ответов (дополнительные лейблы: `status`).
    * `..._request_seconds_{sum,count,bucket}` — histogram времени ответа.
    * `..._bytes_received_{sum,count,bucket}` — histogram размера запроса.
    * `..._bytes_sent_{sum,count,bucket}` — histogram размера ответа.
    * `..._upstream_response_seconds_{sum,count,bucket}` — histogram времени ответа upstream'а (используется сумма времен ответов всех upstream'ов, если их было несколько).
    * `..._lowres_upstream_response_seconds_{sum,count,bucket}` — тоже самое, что предыдущая метрика, только с меньшей детализацией (подходит для визуализации, но не подходит для расчета quantile).
    * `..._upstream_retries_{count,sum}` — количество запросов, при обработке которых были retry бекендов, и сумма retry'ев.
* Для уровня overall собираются следующие метрики:
    * `..._geohash_total` — counter количества запросов с определенным geohash (дополнительные лейблы: `geohash`, `place`).
* Для уровня detail_backend собираются следующие метрики:
    * `..._lowres_upstream_response_seconds` — тоже самое, что аналогичная метрика для overall и detail.
    * `..._responses_total` — counter количества ответов (дополнительный лейбл `status_class`, а не просто `status`).
    *  `..._upstream_bytes_received_sum` — counter суммы размеров ответов backend'а.

На основании собираемых метрик в Grafana строится более сотни графиков, дающие понимание движения трафика: по длительности времени ответа, кодам ответа, повторам (retry) запросов, HTTP-заголовкам, размеру запроса и ответа и др. Данные представлены в нескольких разрезах: по namespace, Ingress-ресурсу, домену, location-у, VHost-ам и другим показателям.


## Интеграция с внешними балансировщиками

Улучшенный Ingress интегрируется с внешними балансировщиками, в том числе: Qrator, Cloudflare, а также балансерами в облаках AWS, GCE и ACS. У каждого из них есть особенности, которые необходимо компенсировать, чтобы балансировщик просто работал как bare metal инсталляция, не требуя дополнительных настроек.

## Другие настройки

Возможность запуска нескольких Nginx Ingress контроллеров на различных нодах и/или портах
Централизованное управление http-авторизацией на ingress-ресурсах
Конфигурирование страниц ошибок
Управление протоколами (HTTP/2, поддержка старых версий TLS, hstsи другое)


> Модуль: [nginx-ingress](/docs/modules/nginx-ingress/)

<!-- 

### Node scaling

Deckhouse может автоматически заказывать ресурсы для ваших приложений. Он интегрирован с Horizontal Pod Autoscaler и позволяет конфигурировать автомасштабирование и характеристики узлов с помощью примитивов kubernetes.

### Security features

Extended версия Deckhouse предоставляет модуль аутентификации (позволяющий работать со всеми ресурсами кластера под единой учётной записью) и модуль авторизации (позволяющий управлять ролями и правами доступа пользователей).
Модуль аутентификации реализован с помощью federated OpenID Connect provider dex, и реализует сквозную авторизацию в служебные сервисы (grafana, prometheus и другие) и API kubernetes с помощью учётных записей ldap, github, saml и многих других.
Модуль авторизации управляет ролями пользователей и тем, какие права имеют пользователи в каждой из ролей.

### Мониторинг

Extended версия Deckhouse предоставляет alerting rules, разработанные и постоянно дорабатываемые нами на основании опыта, полученного в десятках клиентов.
Также Deckhouse содержит prometheus и grafana для сбора и анализа метрик о здоровье кластера.

-->

