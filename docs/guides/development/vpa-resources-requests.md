# Автонастройка ресурсов VPA

## Проблема

* При распределении ресурсов для системных компонентов VerticalPodAutoscaler не учитывает фактически доступные ресурсы на узлах, что может привести к тому, что под не сможет зашедулиться.
* Для статических подов в Control-Plane не выставляются resources requests, что может приводить к вытеснению их с узла.

## Задача

* Сумма запросов ресурсов системных компонентов и компонентов control-plane не должна превышать заданный лимит, который устанавливается при помощи конфига, либо, исходя из свободных ресурсов на узлах.
* Доступные ресурсы должны распределяться справедливо между системными компонентами и компонентами control-plane.

## Реализация

#### Хук global-hooks/node_resources.
Этот хук:
 * Подсчитывает доступные ресурсы узлов.
 * Устанавливает глобальные переменные ```allocatableMilliCpuControlPlane``` и ```allocatableMemoryControlPlane```, определяющие доступные ресурсы для control-plane компонентов (ресурсы для них выставляются статически, без VPA).
 * Устанавливает глобальные переменные ```allocatableMilliCpuMaster``` и ```allocatableMemoryMaster```, определяющие доступные ресурсы для системных компонентов с меткой ```workload-resource-policy.deckhouse.io: master```.
 * Устанавливает глобальные переменные ```allocatableMilliCpuAnyNode``` и ```allocatableMemoryAnyNode```, определяющие доступные ресурсы для системных компонентов с меткой ```workload-resource-policy.deckhouse.io: any-node```.
 
Для этого хука есть переменные, которые можно настроить в конфиге Deckhouse:
* ```controlPlaneRequestsCpu``` - максимальное количество CPU units для компонентов control-plane и для системных компонентов с меткой ```workload-resource-policy.deckhouse.io: master```.
* ```controlPlaneRequestsMemory``` - максимальное количество памяти для компонентов control-plane и для системных компонентов с меткой ```workload-resource-policy.deckhouse.io: master```.
* ```anyNodeRequestsCpu``` - максимальное количество CPU units для системных компонентов с меткой ```workload-resource-policy.deckhouse.io: any-node```.
* ```anyNodeRequestsMemory``` - максимальное количество памяти для системных компонентов с меткой ```workload-resource-policy.deckhouse.io: any-node```.

Расчет доступных ресурсов происходит по следующей схеме:
 * Для компонентов control-plane и для системных компонентов с меткой ```workload-resource-policy.deckhouse.io: master```:
   * Берутся минимальные значения свободных CPU и памяти среди всех master-узлов.
   * Если полученные значения больше захардкоженных значений в хуке (2 ядра, 2Гб памяти), то используются захардкоженные значения.
   * Вычисляется процент от значений, полученных на предыдущем этапе (80%).
   * Если полученные на предыдущем этапе значения больше значений, установленных пользователем в переменных ```controlPlaneRequestsCpu```, ```controlPlaneRequestsMemory```, то берутся значения установленные пользователем.
   * Значения, полученные на предыдущем этапе делятся на две части, соответственно захардкоженным в скрипте процентам (```control_plane_percent```, ```master_percent```).
   * Часть, соответствующая ```control_plane_percent```, отводится для компонентов control-plane.
   * Часть, соответствующая ```master_percent```, отводится для системных компонентов с меткой ```workload-resource-policy.deckhouse.io: master```.
 * Для системных компонентов с меткой ```workload-resource-policy.deckhouse.io: any-node```:
   * Берутся минимальные значения свободных CPU и памяти среди всех узлов.
   * Если полученные значения больше захардкоженных значений в хуке (2 ядра, 2Гб памяти), то используются захардкоженные значения.
   * Вычисляется процент от значений, полученных на предыдущем этапе (20%).
   * Если полученные на предыдущем этапе значения больше значений, установленных пользователем в переменных ```anyNodeRequestsCpu```, ```anyNodeRequestsMemory```, то берутся значения установленные пользователем.
   * Полученное значение  отводится для системных компонентов с меткой ```workload-resource-policy.deckhouse.io: any-node```.
Подрoбнее детали реализации можно посмотреть в комментариях в исходном коде хука.
   
#### Хук 302-vertical-pod-autoscaler/hooks/set_maxallowed
Этот хук:
 * Подсчитывает запрашиваемые ресурсы VPA.
 * Рассчитывает максимально допустимые запросы ресурсов VPA, исходя из доступных ресурсов, установленных в глобальных переменных хуком global-hooks/node_resources.
 * Устанавливает новые значения maxAllowed для ресурсов VPA, исходя из рассчитанных значений.
 
Подрoбнее детали реализации можно посмотреть в комментариях в исходном коде хука.

Для работы хука необходимо:
* Модуль deckhouse virtual-pod-autoscaler должен быть включен.
* Для всех системных компонентов deckhouse были прописаны соответствующие VPA.

Хук поддерживает группирование VPA:

* Master - VPA системных компонентов, которые работают на мастер-узлах. Для включения VPA в эту группу необходимо добавить метку ```workload-resource-policy.deckhouse.io: master``` VPA-ресурсу.
* Any-node - VPA системных компонентов, которые работают на всех узлах. Для включения VPA в эту группу необходимо добавить метку ```workload-resource-policy.deckhouse.io: any-node``` VPA-ресурсу.

Если у VPA-ресурса отсутствуют вышеперечисленные метки, то этот ресурс не включается в подсчет и для него не действует автонастройка resources requests.

#### Темплейт candi/control-plane-kubeadm/kustomize/050-set-requests.yaml

Темплейт предназначен для выставления resources requests для компонентов control-plane.

* kube-controller-manager
* kube-scheduler
* kube-apiserver
* etcd

Ресурсы рассчитываются исходя из максимума, установленного в глобальных переменных ```allocatableMilliCpuControlPlane```, ```allocatableMemoryControlPlane```.
Если эти переменные не установлены (например, при бутстрапе кластера), используются захардкоженные значения в темплейте.
