Модуль node-local-dns
=====================

## Назначение

Разворачивает кеширующий DNS сервер на каждой ноде кластера, экспортирует данные в `Prometheus` для удобного анализа работы DNS в кластере на [Dashboard](#dashboard).

Модуль состоит из оригинального `CoreDNS`, разворачиваемого в DaemonSet на все ноды, с добавлением алгоритма настройки сети и настройки правил iptables для работы fallback (за основу взят [node-cache](https://github.com/kubernetes/dns/blob/master/cmd/node-cache/main.go)).

### Зачем?

Существует ряд проблем в стандартной работе DNS в kubernetes, которые могут привести к неоправданному снижению ключевых показателей работы сервиса:
- Отсутствие кеширования запросов.
- Все DNS запросы из контейнера влекут за собой сетевой запрос к кластерному DNS. Запросы к ресурсам в рамках одной ноды все равно приводят к сетевым запросам.
- Запрос из пода сначала разрешается в кластерных DNS зонах, и только потом отправляется на внешние DNS серверы. Например, запрос на ya.ru будет разрешаться сначала в кластерных зонах, таких как — `cluster.local`, `svc.cluster.local`, `<namespace>.svc.cluster.local` и только после получения отрицательных ответов, т.е. по сути только с более чем 2-го раза, будет разрешаться как надо.

В случае появившихся небольших сетевых задержек, из-за приведенных выше проблем качество сервиса может значительно деградировать.

Одно из решений — поставить DNS сервер на каждую ноду. Для этого и предназначен настоящий модуль, в результате включения которого в целом ускоряется работа с DNS в кластере.

Внешние запросы также будут сначала пытаться разрешаться по цепочке внутренних зон, но только отсутствующие в кеше. При большой нагрузке, такого поведения достаточно для значительного улучшения DNS резолвинга.

## Конфигурация

По умолчанию — **выключен**.

Для использования, нужно **изменить настройки kubelet'ов**.

> **Важно** при добавлении новых узлов в кластер, не забывать их также конфигурировать.

Чтобы начать использовать модуль, необходимо выполнить два шага:
1. Включить модуль добавив в ConfigMap Antiopa:
   ```yaml
   nodeLocalDns: "{}"
   ```
2. Настроить `kubelet` специфичным для вашего типа инсталляции образом (см. далее).

### Конфигурация для Baremetal
Настроить все kubelet'ы в кластере на использование нового DNS через опцию `--cluster-dns=169.254.20.10`.

### Конфигурация для kops
1. Указать в спецификациях всех `InstanceGroups`:
    ```yaml
    spec:
      kubelet:
        clusterDNS: "169.254.20.10"
    ```

2. Перекатить кластер:
    ```
    kops update cluster
    kops rolling-update cluster
    ```

### Конфигурация для aks-engine (Azure)
К сожалению, на `aks-engine` пока нет возможности включить модуль ([следите](https://github.com/deckhouse/deckhouse/issues/418) за обновлениями).

## Grafana dashboard

`Kubernetes / Node Local DNS`, на которой отображены:
- Общие графики. Позволяют оценить в целом картину работы DNS.
- Графики по нодам. В случае идентификации проблем какой-то ноды на общих графиках, позволяют более детально изучить ее данные.
- Графики по upstream. Позволяют оценить работу кластерного DNS и северов узлов, указанных в `/etc/resolv.conf`.

## Как работает

При запуске, модуль выполняет следующие этапы:
1. [Настраивает цепочки и правила iptables](#работа-с-iptables)
1. [Настраивает сетевой интерфейс](#работа-с-сетевой-подсистемой)
1. Запускает со-процесс, который каждые 60 секунд проверяет наличие iptables цепочек/правил и интерфейса с IPv4 адресом
1. [Запускает CoreDNS](#работа-с-coredns)

При завершении, модуль выполняет:
1. Остановку CoreDNS
1. Остановку со-процесса
1. Удаление сетевого интерфейса

### Работа с iptables

Возможны два варианта работы в зависимости от режима работы kube-proxy — `ipvs` и `iptables`.

В обоих вариантах работы создается цепочка `NODE-LOCAL-DNS` в таблице `nat`. Далее, в зависимости от режима работы kube-proxy, генерируется набор правил для fallback'а DNS запросов в локальных контейнерах на кластерный kube-dns.

#### Правила iptables, если kube-proxy работает в режиме ipvs

1. `-A PREROUTING -d 169.254.20.10/32 -p tcp -m tcp --dport 53 -j NODE-LOCAL-DNS`
1. `-A PREROUTING -d 169.254.20.10/32 -p udp -m udp --dport 53 -j NODE-LOCAL-DNS`
1. `-A NODE-LOCAL-DNS -m socket -j RETURN`
1. `-A NODE-LOCAL-DNS -d 169.254.20.10/32 -p tcp -m tcp --dport 53 -j DNAT --to-destination 192.168.0.10:53`
1. `-A NODE-LOCAL-DNS -d 169.254.20.10/32 -p udp -m udp --dport 53 -j DNAT --to-destination 192.168.0.10:53`

* Правила 1 и 2 отправляют пакет на обработку в цепочку `NODE-LOCAL-DNS`.
* Правило 3 выполняет проверку наличия *слушающего* сокета для входящего пакета. Таким образом, когда компонент CoreDNS запустится, правило начнёт отрабатывать (пакет удовлетворит условию правила) и возвратит пакет из цепочки `NODE-LOCAL-DNS`. Пакет продолжит обрабатываться ядром согласно обычных правил iptables, и в итоге достигнет CoreDNS.
* Правила 4 и 5 выполняют DNAT пакета к serviceIP кластерного DNS. Пакет дойдет до этих правил в случае если не запущен CoreDNS и, соответственно, не отработало правило 3 (т.к. пакет не удовлетворит условию правила).

Маршрут прохождения DNS-запроса из контейнера, в случае **запущенного** `node-local-dns`:

![alive](doc/alive-node-local-dns.png)

Маршрут прохождения DNS-запроса из контейнера, в случае **НЕ запущенного** `node-local-dns`:

![dead](doc/dead-node-local-dns.png)


#### Правила iptables, если kube-proxy работает в режиме iptables

1. `-A PREROUTING -d 169.254.20.10/32 -p tcp -m tcp --dport 53 -j NODE-LOCAL-DNS`
1. `-A PREROUTING -d 169.254.20.10/32 -p udp -m udp --dport 53 -j NODE-LOCAL-DNS`
1. `-A NODE-LOCAL-DNS -m socket -j RETURN`
1. `-A NODE-LOCAL-DNS -d 169.254.20.10/32 -j KUBE-MARK-MASQ`
1. `-A NODE-LOCAL-DNS -d 169.254.20.10/32 -p tcp -j KUBE-SVC-ERIFXISQEP7F7OF4`
1. `-A NODE-LOCAL-DNS -d 169.254.20.10/32 -p udp -j KUBE-SVC-TCOU7JCQXEZGVUNU`

* Правила 1 и 2 отправляют пакет на обработку в цепочку `NODE-LOCAL-DNS`.
* Правило 3 выполняет проверку наличия *слушающего* сокета для входящего пакета. Таким образом, когда компонент CoreDNS запустится, правило начнёт отрабатывать (пакет удовлетворит условию правила) и возвратит пакет из цепочки `NODE-LOCAL-DNS`. Пакет продолжит обрабатываться ядром согласно обычных правил iptables, и в итоге достигнет CoreDNS.
* Правило 4 навешивает метку, по которой в POSTROUTING происходит MASQUERADE.
* Правила 5 и 6 отправляют пакет в цепочки, которые сгенерировал `kube-proxy` для сервиса `kube-dns`.
    * **Важное замечание.** Имя цепочек генерируется из сервиса в [таком](https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/iptables/proxier.go#L558-L573) формате (для kube-dns): `kube-system/kube-dns:dnsudp`. Т.к. на всех наших кластерах kube-dns сервис одинаков, никакой логики выбора имени цепочки не предусмотрено.

### Работа с сетевой подсистемой

1. Создаёт интерфейс с типом dummy, не связанный ни с какими физическими интерфейсами
1. Присваивает созданному интерфейсу link-local IPv4 адрес (сейчас — статический `169.254.20.10/32`)

### Работа с CoreDNS

CoreDNS используется [оригинальный](https://coredns.io/), без изменений в основном функционале. Запускается с замонтированным подготовленным конфигом.

Основные характеристики конфига CoreDNS:
1. Кэширование всех запросов
1. Forward DNS запросов, содержаших cluster domain (по-умолчанию, `cluster.local`) и PTR записи, в ClusterIP кластерного DNS
1. Forward всех остальных запросов на указанные в `resolv.conf` DNS серверы
1. Работает с кластерным kube-dns по tcp, что снижает задержки при потере пакетов по сравнению с классическим вариантом работы по udp.
