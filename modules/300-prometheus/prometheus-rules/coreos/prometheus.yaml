- name: coreos.prometheus
  rules:
  - alert: PrometheusConfigReloadFailed
    expr: min(prometheus_config_last_reload_successful) BY (namespace, pod) == 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}
      summary: Reloading Promehteus' configuration failed
#  - alert: PrometheusNotificationQueueRunningFull
#    expr: max(predict_linear(prometheus_notifications_queue_length[5m], 60 * 30) > prometheus_notifications_queue_capacity) BY (namespace, pod)
#    for: 10m
#    labels:
#      severity: warning
#    annotations:
#      description: Prometheus' alert notification queue is running full for {{$labels.namespace}}/{{
#        $labels.pod}}
#      summary: Prometheus' alert notification queue is running full
#  - alert: PrometheusErrorSendingAlerts
#    expr: max(rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
#      > 0.01) BY (namespace, pod, alertmanager)
#    for: 30m
#    labels:
#      severity: warning
#    annotations:
#      description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
#        $labels.pod}} to Alertmanager {{$labels.alertmanager}}
#      summary: Errors while sending alert from Prometheus
#  - alert: PrometheusErrorSendingAlerts
#    expr: max(rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
#      > 0.03) by (namespace, pod, alertmanager)
#    for: 1h
#    labels:
#      severity: critical
#    annotations:
#      description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
#        $labels.pod}} to Alertmanager {{$labels.alertmanager}}
#      summary: Errors while sending alerts from Prometheus
  - alert: PrometheusNotConnectedToAlertmanagers
    expr: min(prometheus_notifications_alertmanagers_discovered) BY (namespace, pod) < 1
    for: 10m
    labels:
      severity: critical
    annotations:
      description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not connected
        to any Alertmanagers
      summary: Prometheus is not connected to any Alertmanagers
  - alert: PrometheusTSDBReloadsFailing
    expr: max(increase(prometheus_tsdb_reloads_failures_total[2h])) BY (namespace, pod) > 0
    for: 12h
    labels:
      severity: warning
    annotations:
      description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod}} had {{$value | humanize}}
        reload failures over the last two hours.'
      summary: Prometheus has issues reloading data blocks from disk
  - alert: PrometheusTSDBCompactionsFailing
    expr: max(increase(prometheus_tsdb_compactions_failed_total[2h])) BY (namespace, pod) > 0
    for: 12h
    labels:
      severity: warning
    annotations:
      description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod}} had {{$value | humanize}}
        compaction failures over the last two hours.'
      summary: Prometheus has issues compacting sample blocks
  - alert: PrometheusTSDBWALCorruptions
    expr: max(prometheus_tsdb_wal_corruptions_total) BY (namespace, pod) > 0
    for: 4h
    labels:
      severity: critical
    annotations:
      description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has a corrupted write-ahead
        log (WAL).'
      summary: Prometheus write-ahead log is corrupted
  - alert: PrometheusNotIngestingSamples
    expr: min(rate(prometheus_tsdb_head_samples_appended_total[5m])) by (namespace, pod) <= 0
    for: 10m
    labels:
      severity: critical
    annotations:
      description: "Prometheus {{ $labels.namespace }}/{{ $labels.pod }} isn't ingesting samples."
      summary: "Prometheus isn't ingesting samples"
