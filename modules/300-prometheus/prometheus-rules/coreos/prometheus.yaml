groups:
- name: coreos.prometheus
  rules:
  - alert: PrometheusConfigReloadFailed
    expr: min(prometheus_config_last_reload_successful) BY (namespace, instance) == 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.instance}}
      summary: Reloading Promehteus' configuration failed
  - alert: PrometheusNotificationQueueRunningFull
    expr: max(predict_linear(prometheus_notifications_queue_length[5m], 60 * 30) > prometheus_notifications_queue_capacity) BY (namespace, instance)
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Prometheus' alert notification queue is running full for {{$labels.namespace}}/{{
        $labels.instance}}
      summary: Prometheus' alert notification queue is running full
  - alert: PrometheusErrorSendingAlerts
    expr: max(rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
      > 0.01) BY (namespace, instance, alertmanager)
    for: 30m
    labels:
      severity: warning
    annotations:
      description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
        $labels.instance}} to Alertmanager {{$labels.alertmanager}}
      summary: Errors while sending alert from Prometheus
  - alert: PrometheusErrorSendingAlerts
    expr: max(rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
      > 0.03) by (namespace, instance, alertmanager)
    for: 1h
    labels:
      severity: critical
    annotations:
      description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
        $labels.instance}} to Alertmanager {{$labels.alertmanager}}
      summary: Errors while sending alerts from Prometheus
  - alert: PrometheusNotConnectedToAlertmanagers
    expr: min(prometheus_notifications_alertmanagers_discovered) BY (namespace, instance) < 1
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Prometheus {{ $labels.namespace }}/{{ $labels.instance }} is not connected
        to any Alertmanagers
      summary: Prometheus is not connected to any Alertmanagers
  - alert: PrometheusTSDBReloadsFailing
    expr: max(increase(prometheus_tsdb_reloads_failures_total[2h])) BY (namespace, instance) > 0
    for: 12h
    labels:
      severity: warning
    annotations:
      description: 'Prometheus {{ $labels.namespace }}/{{ $labels.instance}} had {{$value | humanize}}
        reload failures over the last two hours.'
      summary: Prometheus has issues reloading data blocks from disk
  - alert: PrometheusTSDBCompactionsFailing
    expr: max(increase(prometheus_tsdb_compactions_failed_total[2h])) BY (namespace, instance) > 0
    for: 12h
    labels:
      severity: warning
    annotations:
      description: 'Prometheus {{ $labels.namespace }}/{{ $labels.instance}} had {{$value | humanize}}
        compaction failures over the last two hours.'
      summary: Prometheus has issues compacting sample blocks
  - alert: PrometheusTSDBWALCorruptions
    expr: max(prometheus_tsdb_wal_corruptions_total) BY (namespace, instance) > 0
    for: 4h
    labels:
      severity: warning
    annotations:
      description: 'Prometheus {{ $labels.namespace }}/{{ $labels.instance }} has a corrupted write-ahead
        log (WAL).'
      summary: Prometheus write-ahead log is corrupted
  - alert: PrometheusNotIngestingSamples
    expr: min(rate(prometheus_tsdb_head_samples_appended_total[5m])) by (namespace, instance) <= 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: "Prometheus {{ $labels.namespace }}/{{ $labels.instance }} isn't ingesting samples."
      summary: "Prometheus isn't ingesting samples"
