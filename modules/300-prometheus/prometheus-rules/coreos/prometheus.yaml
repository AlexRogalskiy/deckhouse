- name: coreos.prometheus
  rules:
  - alert: PrometheusConfigReloadFailed
    expr: min(prometheus_config_last_reload_successful) BY (namespace, pod) == 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}
      summary: Reloading Promehteus' configuration failed
  - alert: PrometheusNotificationQueueRunningFull
    expr: |
      max by (namespace, pod) (
        (
          max_over_time(prometheus_notifications_queue_length[5m])
          /
          prometheus_notifications_queue_capacity
        ) * 100
        > 10
      )
    for: 1m
    labels:
      severity: critical
    annotations:
      description: |
        Prometheus' alert notification queue is running full for Prometheus {{$labels.namespace}}/{{$labels.pod}}
        Currently at: {{$value}}%
      summary: Prometheus' alert notification queue is running full
  - alert: PrometheusMadisonErrorSendingAlerts
    expr: |
      max by (prometheus_namespace, prometheus_pod, pod_ip) (
        label_replace(
          label_replace(
            label_replace(
              max(rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
                > 0.01) by (namespace, pod, alertmanager),
              "pod_ip", "$1", "alertmanager", ".*://(.*):.*"),
            "prometheus_namespace", "$1", "namespace", "(.*)"),
          "prometheus_pod", "$1", "pod", "(.*)")
      )
      * on (pod_ip) group_right(prometheus_namespace, prometheus_pod)
      max by (namespace, pod, pod_ip) (kube_pod_info)
      * on (namespace, pod) group_left(label_madison_backend)
      kube_pod_labels{label_madison_backend!=""}
    for: 1m
    labels:
      severity: warning
    annotations:
      description: Errors while sending alerts from Prometheus {{$labels.prometheus_namespace}}/{{ $labels.prometheus_pod}} to madison backend {{$labels.label_madison_backend}} via madison-proxy {{$labels.namespace}}/{{$labels.pod}}
      summary: Errors while sending alert from Prometheus {{$labels.prometheus_namespace}}/{{$labels.prometheus_pod}}
  - alert: PrometheusMadisonErrorSendingAlerts
    expr: |
      max by (prometheus_namespace, prometheus_pod, pod_ip) (
        label_replace(
          label_replace(
            label_replace(
              max(rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
                > 0.05) by (namespace, pod, alertmanager),
              "pod_ip", "$1", "alertmanager", ".*://(.*):.*"),
            "prometheus_namespace", "$1", "namespace", "(.*)"),
          "prometheus_pod", "$1", "pod", "(.*)")
      )
      * on (pod_ip) group_right(prometheus_namespace, prometheus_pod)
      max by (namespace, pod, pod_ip) (kube_pod_info)
      * on (namespace, pod) group_left(label_madison_backend)
      kube_pod_labels{label_madison_backend!=""}
    for: 1m
    labels:
      severity: critical
    annotations:
      description: Errors while sending alerts from Prometheus {{$labels.prometheus_namespace}}/{{ $labels.prometheus_pod}} to madison backend {{$labels.label_madison_backend}} via madison-proxy {{$labels.namespace}}/{{$labels.pod}}
      summary: Errors while sending alert from Prometheus {{$labels.prometheus_namespace}}/{{$labels.prometheus_pod}}
  - alert: PrometheusNotConnectedToMadison
    expr: min(prometheus_notifications_alertmanagers_discovered) BY (namespace, pod) < 1
    for: 3m
    labels:
      severity: critical
    annotations:
      description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not connected
        to Madison
      summary: Prometheus is not connected to Madison
  - alert: PrometheusTSDBReloadsFailing
    expr: max(increase(prometheus_tsdb_reloads_failures_total[2h])) BY (namespace, pod) > 0
    for: 12h
    labels:
      severity: warning
    annotations:
      description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod}} had {{$value | humanize}}
        reload failures over the last two hours.'
      summary: Prometheus has issues reloading data blocks from disk
  - alert: PrometheusTSDBCompactionsFailing
    expr: max(increase(prometheus_tsdb_compactions_failed_total[2h])) BY (namespace, pod) > 0
    for: 12h
    labels:
      severity: warning
    annotations:
      description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod}} had {{$value | humanize}}
        compaction failures over the last two hours.'
      summary: Prometheus has issues compacting sample blocks
  - alert: PrometheusTSDBWALCorruptions
    expr: max(prometheus_tsdb_wal_corruptions_total) BY (namespace, pod) > 0
    for: 4h
    labels:
      severity: critical
    annotations:
      description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has a corrupted write-ahead
        log (WAL).'
      summary: Prometheus write-ahead log is corrupted
  - alert: PrometheusNotIngestingSamples
    expr: min(rate(prometheus_tsdb_head_samples_appended_total[5m])) by (namespace, pod) <= 0
    for: 10m
    labels:
      severity: critical
    annotations:
      description: "Prometheus {{ $labels.namespace }}/{{ $labels.pod }} isn't ingesting samples."
      summary: "Prometheus isn't ingesting samples"
