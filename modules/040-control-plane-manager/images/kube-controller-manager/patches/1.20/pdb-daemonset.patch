From 5f0a0b6b62090f45b39021aba4d8c258d6faa596 Mon Sep 17 00:00:00 2001
From: Eugene <evgeny.shevchenko@flant.com>
Date: Thu, 14 Jan 2021 06:05:57 +0300
Subject: [PATCH] add DaemonSet support in PDB

via .Status.DesiredNumberScheduled as the desired number of pods
---
 cmd/kube-controller-manager/app/policy.go    |  1 +
 pkg/controller/disruption/disruption.go      | 31 ++++++-
 pkg/controller/disruption/disruption_test.go | 87 ++++++++++++++++++++
 3 files changed, 117 insertions(+), 2 deletions(-)

diff --git a/cmd/kube-controller-manager/app/policy.go b/cmd/kube-controller-manager/app/policy.go
index a3aa4aae210..7a06c79de7c 100644
--- a/cmd/kube-controller-manager/app/policy.go
+++ b/cmd/kube-controller-manager/app/policy.go
@@ -64,6 +64,7 @@ func startDisruptionController(ctx ControllerContext) (http.Handler, bool, error
 		ctx.InformerFactory.Apps().V1().ReplicaSets(),
 		ctx.InformerFactory.Apps().V1().Deployments(),
 		ctx.InformerFactory.Apps().V1().StatefulSets(),
+		ctx.InformerFactory.Apps().V1().DaemonSets(),
 		client,
 		ctx.RESTMapper,
 		scaleClient,
diff --git a/pkg/controller/disruption/disruption.go b/pkg/controller/disruption/disruption.go
index 11642b422ec..bf636816a9d 100644
--- a/pkg/controller/disruption/disruption.go
+++ b/pkg/controller/disruption/disruption.go
@@ -47,6 +47,7 @@ import (
 	"k8s.io/client-go/tools/cache"
 	"k8s.io/client-go/tools/record"
 	"k8s.io/client-go/util/workqueue"
+
 	podutil "k8s.io/kubernetes/pkg/api/v1/pod"
 	"k8s.io/kubernetes/pkg/controller"
 
@@ -90,6 +91,9 @@ type DisruptionController struct {
 	ssLister       appsv1listers.StatefulSetLister
 	ssListerSynced cache.InformerSynced
 
+	dsLister       appsv1listers.DaemonSetLister
+	dsListerSynced cache.InformerSynced
+
 	// PodDisruptionBudget keys that need to be synced.
 	queue        workqueue.RateLimitingInterface
 	recheckQueue workqueue.DelayingInterface
@@ -118,6 +122,7 @@ func NewDisruptionController(
 	rsInformer appsv1informers.ReplicaSetInformer,
 	dInformer appsv1informers.DeploymentInformer,
 	ssInformer appsv1informers.StatefulSetInformer,
+	dsInformer appsv1informers.DaemonSetInformer,
 	kubeClient clientset.Interface,
 	restMapper apimeta.RESTMapper,
 	scaleNamespacer scaleclient.ScalesGetter,
@@ -162,6 +167,9 @@ func NewDisruptionController(
 	dc.ssLister = ssInformer.Lister()
 	dc.ssListerSynced = ssInformer.Informer().HasSynced
 
+	dc.dsLister = dsInformer.Lister()
+	dc.dsListerSynced = dsInformer.Informer().HasSynced
+
 	dc.mapper = restMapper
 	dc.scaleNamespacer = scaleNamespacer
 
@@ -174,7 +182,7 @@ func NewDisruptionController(
 // resources directly and only fall back to the scale subresource when needed.
 func (dc *DisruptionController) finders() []podControllerFinder {
 	return []podControllerFinder{dc.getPodReplicationController, dc.getPodDeployment, dc.getPodReplicaSet,
-		dc.getPodStatefulSet, dc.getScaleController}
+		dc.getPodStatefulSet, dc.getPodDaemonSet, dc.getScaleController}
 }
 
 var (
@@ -182,6 +190,7 @@ var (
 	controllerKindSS  = apps.SchemeGroupVersion.WithKind("StatefulSet")
 	controllerKindRC  = v1.SchemeGroupVersion.WithKind("ReplicationController")
 	controllerKindDep = v1beta1.SchemeGroupVersion.WithKind("Deployment")
+	controllerKindDS  = apps.SchemeGroupVersion.WithKind("DaemonSet")
 )
 
 // getPodReplicaSet finds a replicaset which has no matching deployments.
@@ -258,6 +267,24 @@ func (dc *DisruptionController) getPodDeployment(controllerRef *metav1.OwnerRefe
 	return &controllerAndScale{deployment.UID, *(deployment.Spec.Replicas)}, nil
 }
 
+// getPodDaemonSet returns the daemonset referenced by the provided controllerRef.
+func (dc *DisruptionController) getPodDaemonSet(controllerRef *metav1.OwnerReference, namespace string) (*controllerAndScale, error) {
+	ok, err := verifyGroupKind(controllerRef, controllerKindDS.Kind, []string{"apps"})
+	if !ok || err != nil {
+		return nil, err
+	}
+	ds, err := dc.dsLister.DaemonSets(namespace).Get(controllerRef.Name)
+	if err != nil {
+		// The only possible error is NotFound, which is ok here.
+		return nil, nil
+	}
+	if ds.UID != controllerRef.UID {
+		return nil, nil
+	}
+
+	return &controllerAndScale{ds.UID, ds.Status.DesiredNumberScheduled}, nil
+}
+
 func (dc *DisruptionController) getPodReplicationController(controllerRef *metav1.OwnerReference, namespace string) (*controllerAndScale, error) {
 	ok, err := verifyGroupKind(controllerRef, controllerKindRC.Kind, []string{""})
 	if !ok || err != nil {
@@ -330,7 +357,7 @@ func (dc *DisruptionController) Run(stopCh <-chan struct{}) {
 	klog.Infof("Starting disruption controller")
 	defer klog.Infof("Shutting down disruption controller")
 
-	if !cache.WaitForNamedCacheSync("disruption", stopCh, dc.podListerSynced, dc.pdbListerSynced, dc.rcListerSynced, dc.rsListerSynced, dc.dListerSynced, dc.ssListerSynced) {
+	if !cache.WaitForNamedCacheSync("disruption", stopCh, dc.podListerSynced, dc.pdbListerSynced, dc.rcListerSynced, dc.rsListerSynced, dc.dListerSynced, dc.ssListerSynced, dc.dsListerSynced) {
 		return
 	}
 
diff --git a/pkg/controller/disruption/disruption_test.go b/pkg/controller/disruption/disruption_test.go
index dbda1fdaece..d0433befdbd 100644
--- a/pkg/controller/disruption/disruption_test.go
+++ b/pkg/controller/disruption/disruption_test.go
@@ -104,6 +104,7 @@ type disruptionController struct {
 	rsStore  cache.Store
 	dStore   cache.Store
 	ssStore  cache.Store
+	dsStore  cache.Store
 
 	coreClient  *fake.Clientset
 	scaleClient *scalefake.FakeScaleClient
@@ -132,6 +133,7 @@ func newFakeDisruptionController() (*disruptionController, *pdbStates) {
 		informerFactory.Apps().V1().ReplicaSets(),
 		informerFactory.Apps().V1().Deployments(),
 		informerFactory.Apps().V1().StatefulSets(),
+		informerFactory.Apps().V1().DaemonSets(),
 		coreClient,
 		testrestmapper.TestOnlyStaticRESTMapper(scheme),
 		fakeScaleClient,
@@ -143,6 +145,7 @@ func newFakeDisruptionController() (*disruptionController, *pdbStates) {
 	dc.rsListerSynced = alwaysReady
 	dc.dListerSynced = alwaysReady
 	dc.ssListerSynced = alwaysReady
+	dc.dsListerSynced = alwaysReady
 
 	informerFactory.Start(context.TODO().Done())
 	informerFactory.WaitForCacheSync(nil)
@@ -155,6 +158,7 @@ func newFakeDisruptionController() (*disruptionController, *pdbStates) {
 		informerFactory.Apps().V1().ReplicaSets().Informer().GetStore(),
 		informerFactory.Apps().V1().Deployments().Informer().GetStore(),
 		informerFactory.Apps().V1().StatefulSets().Informer().GetStore(),
+		informerFactory.Apps().V1().DaemonSets().Informer().GetStore(),
 		coreClient,
 		fakeScaleClient,
 	}, ps
@@ -241,6 +245,13 @@ func updatePodOwnerToSs(t *testing.T, pod *v1.Pod, ss *apps.StatefulSet) {
 	pod.OwnerReferences = append(pod.OwnerReferences, controllerReference)
 }
 
+func updatePodOwnerToDs(t *testing.T, pod *v1.Pod, ds *apps.DaemonSet) {
+	var controllerReference metav1.OwnerReference
+	var trueVar = true
+	controllerReference = metav1.OwnerReference{UID: ds.UID, APIVersion: controllerKindDS.GroupVersion().String(), Kind: controllerKindDS.Kind, Name: ds.Name, Controller: &trueVar}
+	pod.OwnerReferences = append(pod.OwnerReferences, controllerReference)
+}
+
 func newPod(t *testing.T, name string) (*v1.Pod, string) {
 	pod := &v1.Pod{
 		TypeMeta: metav1.TypeMeta{APIVersion: "v1"},
@@ -364,6 +375,32 @@ func newStatefulSet(t *testing.T, size int32) (*apps.StatefulSet, string) {
 	return ss, ssName
 }
 
+func newDaemonSet(t *testing.T, size int32) (*apps.DaemonSet, string) {
+	ds := &apps.DaemonSet{
+		TypeMeta: metav1.TypeMeta{APIVersion: "v1"},
+		ObjectMeta: metav1.ObjectMeta{
+			UID:             uuid.NewUUID(),
+			Name:            "foobar",
+			Namespace:       metav1.NamespaceDefault,
+			ResourceVersion: "18",
+			Labels:          fooBar(),
+		},
+		Spec: apps.DaemonSetSpec{
+			Selector: newSelFooBar(),
+		},
+		Status: apps.DaemonSetStatus{
+			DesiredNumberScheduled: size,
+		},
+	}
+
+	dsName, err := controller.KeyFunc(ds)
+	if err != nil {
+		t.Fatalf("Unexpected error naming DaemonSet %q: %v", ds.Name, err)
+	}
+
+	return ds, dsName
+}
+
 func update(t *testing.T, store cache.Store, obj interface{}) {
 	if err := store.Update(obj); err != nil {
 		t.Fatalf("Could not add %+v to %+v: %v", obj, store, err)
@@ -691,6 +728,37 @@ func TestStatefulSetController(t *testing.T) {
 	}
 }
 
+func TestDaemonSetController(t *testing.T) {
+	labels := map[string]string{
+		"foo": "bar",
+		"baz": "quux",
+	}
+
+	dc, ps := newFakeDisruptionController()
+
+	// 34% should round up to 2
+	pdb, pdbName := newMinAvailablePodDisruptionBudget(t, intstr.FromString("34%"))
+	add(t, dc.pdbStore, pdb)
+	ds, _ := newDaemonSet(t, 3)
+	add(t, dc.dsStore, ds)
+	dc.sync(pdbName)
+
+	ps.VerifyPdbStatus(t, pdbName, 0, 0, 0, 0, map[string]metav1.Time{})
+
+	for i := int32(0); i < 3; i++ {
+		pod, _ := newPod(t, fmt.Sprintf("foobar %d", i))
+		updatePodOwnerToDs(t, pod, ds)
+		pod.Labels = labels
+		add(t, dc.podStore, pod)
+		dc.sync(pdbName)
+		if i < 2 {
+			ps.VerifyPdbStatus(t, pdbName, 0, i+1, 2, 3, map[string]metav1.Time{})
+		} else {
+			ps.VerifyPdbStatus(t, pdbName, 1, 3, 2, 3, map[string]metav1.Time{})
+		}
+	}
+}
+
 func TestTwoControllers(t *testing.T) {
 	// Most of this test is in verifying intermediate cases as we define the
 	// three controllers and create the pods.
@@ -844,6 +912,8 @@ func TestBasicFinderFunctions(t *testing.T) {
 	add(t, dc.rcStore, rc)
 	ss, _ := newStatefulSet(t, 14)
 	add(t, dc.ssStore, ss)
+	ds, _ := newDaemonSet(t, 13)
+	add(t, dc.dsStore, ds)
 
 	testCases := map[string]struct {
 		finderFunc    podControllerFinder
@@ -905,6 +975,23 @@ func TestBasicFinderFunctions(t *testing.T) {
 			uid:        ss.UID,
 			findsScale: false,
 		},
+		"daemonset controller with extensions group": {
+			finderFunc:    dc.getPodDaemonSet,
+			apiVersion:    "apps/v1",
+			kind:          controllerKindDS.Kind,
+			name:          ds.Name,
+			uid:           ds.UID,
+			findsScale:    true,
+			expectedScale: 13,
+		},
+		"daemonset controller with invalid kind": {
+			finderFunc: dc.getPodDaemonSet,
+			apiVersion: "apps/v1",
+			kind:       controllerKindRS.Kind,
+			name:       ds.Name,
+			uid:        ds.UID,
+			findsScale: false,
+		},
 	}
 
 	for tn, tc := range testCases {
-- 
2.29.2

