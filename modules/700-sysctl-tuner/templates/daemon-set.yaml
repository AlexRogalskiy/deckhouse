{{- if (.Values.global.enabledModules | has "vertical-pod-autoscaler-crd") }}
---
apiVersion: autoscaling.k8s.io/v1beta2
kind: VerticalPodAutoscaler
metadata:
  name: sysctl-tuner
  namespace: d8-system
{{ include "helm_lib_module_labels" (list . (dict "app" "sysctl-tuner")) | indent 2 }}
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: DaemonSet
    name: sysctl-tuner
  updatePolicy:
    updateMode: "Auto"
{{- end }}
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: sysctl-tuner
  namespace: d8-system
{{ include "helm_lib_module_labels" (list . (dict "app" "sysctl-tuner")) | indent 2 }}
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      heritage: deckhouse
      module: {{ $.Chart.Name }}
  template:
    metadata:
      labels:
        heritage: deckhouse
        module: {{ $.Chart.Name }}
    spec:
      imagePullSecrets:
      - name: deckhouse-registry
      containers:
      - command:
        - sh
        - -c
        - |
          while true; do
            CPU_NUM=`cat /proc/cpuinfo | grep -E '^processor\s+:\s+[0-9]+$' | wc -l` ;
            CONNTRACK_BY_CPU=$(( $CPU_NUM * {{ .Values.sysctlTuner.conntrackMaxPerCore }} )) ;
            NF_CONNTRACK_MAX=$(( $CONNTRACK_BY_CPU > {{ .Values.sysctlTuner.conntrackMin }} ? $CONNTRACK_BY_CPU : {{ .Values.sysctlTuner.conntrackMin }} )) ;
            sysctl -w net.netfilter.nf_conntrack_max=$NF_CONNTRACK_MAX ; # устанавливаем лимит количества conntrack-ов
            sysctl -w net.nf_conntrack_max=$NF_CONNTRACK_MAX ;           #
            echo $(( $NF_CONNTRACK_MAX / 4 )) > /sys/module/nf_conntrack/parameters/hashsize ; # устанавливаем пропорциональный размер хеш-таблицы для поиска по коннтракам

            # http://www.brendangregg.com/blog/2017-12-31/reinvent-netflix-ec2-tuning.html
            sysctl -w vm.swappiness=0 ;
            sysctl -w net.core.somaxconn=1000 ;
            sysctl -w net.core.netdev_max_backlog=5000 ; # увеличим бэклог пакетов, забранных с ring buffer'а сетевой карты, но ещё не переданные вверх по сетевому стеку ядра
            sysctl -w net.core.rmem_max=16777216 ;
            sysctl -w net.core.wmem_max=16777216 ;
            sysctl -w net.ipv4.tcp_wmem="4096 12582912 16777216" ;
            sysctl -w net.ipv4.tcp_rmem="4096 12582912 16777216" ;
            sysctl -w net.ipv4.tcp_max_syn_backlog=8096 ;
            sysctl -w net.ipv4.tcp_no_metrics_save=1 ; # не кэшируем TCP метрики для последующих коннектов, использующих тот же (dst_ip, src_ip, dst_port, src_port) tuple, ибо вредно и не нужно в современных WAN сетях
            sysctl -w net.ipv4.tcp_slow_start_after_idle=0 ; # не нужно в современных сетях, ибо начинает агрессивно снижать TCP cwnd на idle коннектах
            sysctl -w net.ipv4.tcp_tw_reuse=1 ; # безопасная опция, позволяющая переиспользовать TIME-WAIT сокет в исходящем соединении
            sysctl -w net.ipv4.ip_local_port_range="10240 65535" ;
            sysctl -w net.ipv4.neigh.default.gc_thresh1=16384 ; #fix neighbour: arp_cache: neighbor table overflow!
            sysctl -w net.ipv4.neigh.default.gc_thresh2=28672 ;
            sysctl -w net.ipv4.neigh.default.gc_thresh3=32768 ;
            sysctl -w vm.dirty_ratio=80 ; # включаем синхронный writeback dirty страниц как можно позже
            sysctl -w vm.dirty_background_ratio=5 ; # включаем параллельный writeback как можно раньше
            sysctl -w vm.dirty_expire_centisecs=12000 ; # через 12 секунд делаем writeback dirty страниц
            sysctl -w fs.file-max=1000000 ;
            sysctl -w vm.min_free_kbytes=131072 ; # увеличиваем безопасный лимит для немедленных аллокаций страниц в ядре (Jumbo Frames, разные IRQ handlers)
            sysctl -w kernel.numa_balancing=0 ; # отключаем чересчур умный балансировщик NUMA нод, чтобы не было провисаний. NUMA affinity лучше настраивать заранее и по-другому
            sysctl -w fs.inotify.max_user_watches=524288 # Увеличим inotify (https://github.com/guard/listen/wiki/Increasing-the-amount-of-inotify-watchers#the-technical-details)
            sysctl -w fs.inotify.max_user_instances=5120
            sysctl -w kernel.pid_max=2000000
            echo 256 | tee /sys/block/*/queue/nr_requests >/dev/null ; # помещаем больше в очередь запросов, повышаем throughput
            echo 256 | tee /sys/block/*/queue/read_ahead_kb >/dev/null ; # самая спорная вещь, Netflix рекомендует чуть-чуть увеличить, но надо тестить на разных сетапах, это число выглядит безопасным
            echo never | tee /sys/kernel/mm/transparent_hugepage/enabled >/dev/null ;
            echo never | tee /sys/kernel/mm/transparent_hugepage/defrag >/dev/null ;
            echo 0 | tee /sys/kernel/mm/transparent_hugepage/use_zero_page >/dev/null ;
            echo 0 | tee /sys/kernel/mm/transparent_hugepage/khugepaged/defrag >/dev/null ;
            sleep 600 ;
          done
        image: {{ $.Values.global.modulesImages.registry }}/common/busybox:{{ $.Values.global.modulesImages.tags.common.busybox }}
        name: busybox
        resources: {}
        securityContext:
          privileged: true
      hostNetwork: true
      hostPID: true
      hostIPC: true
{{- include "helm_lib_priority_class" (tuple . "cluster-critical") | indent 6 }}
      tolerations:
      - operator: Exists
