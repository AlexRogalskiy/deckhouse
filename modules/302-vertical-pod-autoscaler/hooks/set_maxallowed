#!/bin/bash
# Overview:
#   1. All system components require resource requests, managed by vpa.
#   2. Sum of all resource requests should not exceed manually configured resources limits.
#   3. We expect that resources limits to be allocated fairly between vpa requests.
# We have 3 groups of resources:
#   1. ControlPlane - resources for pods in control-plane (kube-controller-manager, kube-scheduler, kube-apiserver, etcd).
#   2. Master - vpa resources, working on master nodes (label "workload-resource-policy.deckhouse.io: master").
#   3. EveryNode - vpa resources, working on every node (label "workload-resource-policy.deckhouse.io: every-node").
# Calculate steps:
#   1. We calculate sum of uncappedTargets requests for all vpa resources in Master group, and proportionally sets MaxAllowed values for this resources,
#      based on resources requests from global config for Master group.
#   2. We calculate sum of uncappedTargets requests for all vpa resources in EveryNode group, and proportionally sets MaxAllowed values for this resources,
#      based on resources requests from global config for EveryNode group.

source /deckhouse/shell_lib.sh

function __config__() {
    cat << "EOF"
    configVersion: v1
    beforeHelm: 20
    settings:
      executionMinInterval: 30s
      executionBurst: 1
    schedule:
    - name: vpacrontab
      group: vpa
      crontab: "0 */6 * * *"
    kubernetes:
    - name: vpa
      group: vpa
      keepFullObjectsInMemory: false
      apiVersion: autoscaling.k8s.io/v1
      kind: VerticalPodAutoscaler
      executeHookOnEvent: []
      labelSelector:
        matchLabels:
          heritage: deckhouse
      jqFilter: |
        {
          namespace: .metadata.namespace,
          name: .metadata.name,
          labels: .metadata.labels,
          recommendation: .status.recommendation
        }
    - name: vpawithoutrecommendations
      group: vpa
      keepFullObjectsInMemory: false
      apiVersion: autoscaling.k8s.io/v1
      kind: VerticalPodAutoscaler
      executeHookOnEvent: ["Added", "Modified"]
      labelSelector:
        matchLabels:
          heritage: deckhouse
      jqFilter: |
        .status.recommendation.containerRecommendations // [] | any(.uncappedTarget)
EOF
}

function __main__() {

  resources_requests_millicpu_master="$(values::get --required "global.modules.resourcesRequests.internal.milliCpuMaster")"
  resources_requests_memory_master="$(values::get --required "global.modules.resourcesRequests.internal.memoryMaster")"
  resources_requests_millicpu_every_node="$(values::get --required "global.modules.resourcesRequests.internal.milliCpuEveryNode")"
  resources_requests_memory_every_node="$(values::get --required "global.modules.resourcesRequests.internal.memoryEveryNode")"

  # Parse vpa resources
  # shellcheck disable=SC2016
  jq_result="$(context::jq -r '.snapshots.vpa[] |
  select(.filterResult and (.filterResult.labels."workload-resource-policy.deckhouse.io" == "every-node" or
  .filterResult.labels."workload-resource-policy.deckhouse.io" == "master")) |
  .filterResult | .namespace as $namespace | .name as $name | .labels."workload-resource-policy.deckhouse.io" as $lbvalue |
  .recommendation.containerRecommendations[]? |
  {namespace: $namespace, name: $name ,containerName: .containerName, cpu: .uncappedTarget.cpu, memory: .uncappedTarget.memory, label: $lbvalue }')"

  # if jq_result is empty, exit
  [ -n "$jq_result" ] || return 0

  mapfile -t namespace_array <<< "$(jq -rec ' .namespace' <<< "$jq_result")"
  mapfile -t name_array <<< "$(jq -rec ' .name'  <<< "$jq_result")"
  mapfile -t container_name_array <<< "$(jq -rec ' .containerName' <<< "$jq_result")"
  mapfile -t cpu_array <<< "$(jq -rec ' .cpu' <<< "$jq_result")"
  mapfile -t memory_array <<< "$(jq -rec ' .memory' <<< "$jq_result")"
  mapfile -t label_array <<< "$(jq -rec ' .label' <<< "$jq_result")"

  # convert cpu to milli and memory to bytes
  for ((i=0; i<${#container_name_array[@]}; i++)); do
   cpu_array[$i]="$(tools::dk_convert --milli "${cpu_array[$i]}")"
   memory_array[$i]="$(tools::dk_convert "${memory_array[$i]}")"
  done

  cpu_expr_master=""
  memory_expr_master=""
  cpu_expr_every_node=""
  memory_expr_every_node=""

  # Get summary requests for master group and every-node group
  for ((i=0; i<${#container_name_array[@]}; i++)); do
    case ${label_array[$i]} in
      master)
        cpu_expr_master+=${cpu_array[$i]}" "
        memory_expr_master+=${memory_array[$i]}" "
        ;;
      every-node)
        cpu_expr_every_node+=${cpu_array[$i]}" "
        memory_expr_every_node+=${memory_array[$i]}" "
        ;;
    esac
  done
  summary_millicpu_master="$(jq -sre 'join("+")' <<< "$cpu_expr_master" | bc)"
  summary_memory_master="$(jq -sre 'join("+")' <<< "$memory_expr_master" | bc)"
  summary_millicpu_every_node="$(jq -sre 'join("+")' <<< "$cpu_expr_every_node" | bc)"
  summary_memory_every_node="$(jq -sre 'join("+")' <<< "$memory_expr_every_node" | bc)"

  # patch vpa resources
  declare -A jq_patch

  for ((i=0; i<${#container_name_array[@]}; i++)); do
   # Compute limits for VPA
    case ${label_array[$i]} in
      master)
        vpa_millicpu="$(bc <<< "$resources_requests_millicpu_master * ${cpu_array[$i]} / $summary_millicpu_master" )"
        vpa_memory="$(bc <<< "$resources_requests_memory_master * ${memory_array[$i]} / $summary_memory_master" )"
        ;;
      every-node)
        vpa_millicpu="$(bc <<< "$resources_requests_millicpu_every_node * ${cpu_array[$i]} / $summary_millicpu_every_node" )"
        vpa_memory="$(bc <<< "$resources_requests_memory_every_node * ${memory_array[$i]} / $summary_memory_every_node" )"
        ;;
    esac

   jq_patch["${namespace_array[$i]} ${name_array[$i]}"]+="{ containerName: \"${container_name_array[$i]}\", maxAllowed: { cpu: \"${vpa_millicpu}m\", memory: \"${vpa_memory}\" }},"
  done

  for key in "${!jq_patch[@]}"; do
    kubernetes::patch_jq "$( cut -d " " -f1 <<< "$key" )" "verticalpodautoscaler/$( cut -d " " -f2 <<< "$key" )" ". | .spec.resourcePolicy.containerPolicies = [ ${jq_patch[$key]%?} ]"
  done

}

hook::run "$@"
