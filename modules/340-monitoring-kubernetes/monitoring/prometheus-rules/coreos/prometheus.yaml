- name: coreos.prometheus
  rules:
  - alert: PrometheusConfigReloadFailed
    expr: min(prometheus_config_last_reload_successful) BY (namespace, pod) == 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}
      summary: Reloading Promehteus' configuration failed
  - alert: PrometheusNotificationQueueRunningFull
    expr: |
      max by (namespace, pod) (
        (
          max_over_time(prometheus_notifications_queue_length[5m])
          /
          prometheus_notifications_queue_capacity
        ) * 100
        > 10
      )
    for: 1m
    labels:
      severity: critical
    annotations:
      description: |
        Prometheus' alert notification queue is running full for Prometheus {{$labels.namespace}}/{{$labels.pod}}
        Currently at: {{$value}}%
      summary: Prometheus' alert notification queue is running full
  - alert: PrometheusMadisonErrorSendingAlerts
    expr: |
      max by (prometheus_namespace, prometheus_pod, pod_ip) (
        label_replace(
          label_replace(
            label_replace(
              max(rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
                > 0.01) by (namespace, pod, alertmanager),
              "pod_ip", "$1", "alertmanager", ".*://(.*):.*"),
            "prometheus_namespace", "$1", "namespace", "(.*)"),
          "prometheus_pod", "$1", "pod", "(.*)")
      )
      * on (pod_ip) group_right(prometheus_namespace, prometheus_pod)
      max by (namespace, pod, pod_ip) (kube_pod_info)
      * on (namespace, pod) group_left(label_madison_backend)
      kube_pod_labels{label_madison_backend!=""}
    for: 1m
    labels:
      severity: warning
    annotations:
      description: Errors while sending alerts from Prometheus {{$labels.prometheus_namespace}}/{{ $labels.prometheus_pod}} to madison backend {{$labels.label_madison_backend}} via madison-proxy {{$labels.namespace}}/{{$labels.pod}}
      summary: Errors while sending alert from Prometheus {{$labels.prometheus_namespace}}/{{$labels.prometheus_pod}}
  - alert: PrometheusMadisonErrorSendingAlerts
    expr: |
      max by (prometheus_namespace, prometheus_pod, pod_ip) (
        label_replace(
          label_replace(
            label_replace(
              max(rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
                > 0.05) by (namespace, pod, alertmanager),
              "pod_ip", "$1", "alertmanager", ".*://(.*):.*"),
            "prometheus_namespace", "$1", "namespace", "(.*)"),
          "prometheus_pod", "$1", "pod", "(.*)")
      )
      * on (pod_ip) group_right(prometheus_namespace, prometheus_pod)
      max by (namespace, pod, pod_ip) (kube_pod_info)
      * on (namespace, pod) group_left(label_madison_backend)
      kube_pod_labels{label_madison_backend!=""}
    for: 1m
    labels:
      severity: critical
    annotations:
      description: Errors while sending alerts from Prometheus {{$labels.prometheus_namespace}}/{{ $labels.prometheus_pod}} to madison backend {{$labels.label_madison_backend}} via madison-proxy {{$labels.namespace}}/{{$labels.pod}}
      summary: Errors while sending alert from Prometheus {{$labels.prometheus_namespace}}/{{$labels.prometheus_pod}}
  - alert: PrometheusNotConnectedToMadison
    expr: min(prometheus_notifications_alertmanagers_discovered) BY (namespace, pod) < 1
    for: 3m
    labels:
      severity: critical
    annotations:
      description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not connected
        to Madison
      summary: Prometheus is not connected to Madison
  - alert: PrometheusTSDBReloadsFailing
    expr: max(increase(prometheus_tsdb_reloads_failures_total[2h])) BY (namespace, pod) > 0
    for: 12h
    labels:
      severity: warning
    annotations:
      description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod}} had {{$value | humanize}}
        reload failures over the last two hours.'
      summary: Prometheus has issues reloading data blocks from disk
  - alert: PrometheusTSDBCompactionsFailing
    expr: max(increase(prometheus_tsdb_compactions_failed_total[2h])) BY (namespace, pod) > 0
    for: 12h
    labels:
      severity: warning
    annotations:
      description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod}} had {{$value | humanize}}
        compaction failures over the last two hours.'
      summary: Prometheus has issues compacting sample blocks
  - alert: PrometheusTSDBWALCorruptions
    expr: max(prometheus_tsdb_wal_corruptions_total) BY (namespace, pod) > 0
    for: 4h
    labels:
      severity: critical
    annotations:
      description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has a corrupted write-ahead
        log (WAL).'
      summary: Prometheus write-ahead log is corrupted
  - alert: PrometheusNotIngestingSamples
    expr: min(rate(prometheus_tsdb_head_samples_appended_total[5m])) by (namespace, pod) <= 0
    for: 10m
    labels:
      severity: critical
    annotations:
      description: "Prometheus {{ $labels.namespace }}/{{ $labels.pod }} isn't ingesting samples."
      summary: "Prometheus isn't ingesting samples"
  - alert: PrometheusOperatorDown
    expr: absent(up{job="prometheus-operator"} == 1)
    for: 1h
    labels:
      impact: negligible
      likelihood: certain
    annotations:
      polk_flant_com_markup_format: markdown
      description: |-
        Под с prometheus-operator недоступен.
        В кластере не могут примениться новые настройки `Prometheus`, `PrometheusRules`, `ServiceMonitor`, но при этом все уже созданные и сконфигурированные элементы работают корректно.
        Данная проблема никак не повлияет на работу алертов или мониторинга в переспективах небольшого промежутка времени (нескольких дней).

        Куда смотреть:
        1. Посмотреть информацию о деплойменте: `kubectl -n d8-operator-prometheus describe deploy prometheus-operator`
        2. Посмотреть состояние пода и понять, почему он не запущен: `kubectl -n d8-operator-prometheus describe pod -l app=prometheus-operator`
      summary: >
        Prometheus operator is down
  - alert: PrometheusLongtermDown
    expr: absent(up{job="prometheus", service="prometheus-longterm"} == 1)
    for: 15m
    labels:
      impact: negligible
      likelihood: certain
    annotations:
      polk_flant_com_markup_format: markdown
      description: |-
        Под с Prometheus longterm не запущен.
        Данный Prometheus используется только для отображения исторических данных и его недоступность может быть совсем некритичной, однако если он будет долгое время недоступен, в будущем вы не сможете посмотреть статистику.

        Чаще всего у данного пода проблемы из-за недоступности диска (например, под переехал на ноду, где диск не цепляется).

        Куда следует смотреть:
        1. Посмотреть информацию о Statefulset: `kubectl -n d8-monitoring describe statefulset prometheus-longterm`
        2. В каком статусе находится его PVC (если он используется): `kubectl -n d8-monitoring describe pvc prometheus-longterm-db-prometheus-longterm-0`
        3. В каком состоянии находится сам под: `kubectl -n d8-monitoring describe pod prometheus-longterm-0`
      summary: >
        Longterm prometheus is down
  - alert: TricksterDown
    expr: absent(up{job="trickster"} == 1)
    for: 5m
    laels:
      impact: critical
      likelihood: certain
    annotations:
      polk_flant_com_markup_format: markdown
      description: |-
        Под trickster (компонент для кеширования запросов к Prometheus) недоступен. Данный компонент используют:
        `prometheus-metrics-adapter` — мы остаемся без работающего HPA (автоскейлинг) и не можем посмотреть потребление ресурсов с помощью `kubectl`.
        `vertical-pod-autoscaler` — для него этот инцидент не так страшен, так как VPA смотрит историю потребления за 8 дней.
        `grafana` — по-умолчанию все дашборды используют trickster для кеширования запросов к Prometheus. Можно забирать данные напрямую из Prometheus, минуя trickster, однако это может привести к повышенному потреблению памяти Prometheus и, соответственно, к недоступности.

        Куда смотреть:
        1. Информация о deployment: `kubectl -n d8-monitoring describe deployment trickster`
        2. Информация о pod: `kubectl -n d8-monitoring describe pod -l app=trickster`
        3. Чаще всего trickster становится недоступным из-за проблем с самим Prometheus, так как readinessProbe trickster'а проверяет доступность Prometheus. Поэтому, убедитесь, что prometheus работает: `kubectl -n d8-monitoring describe pod -l app=prometheus,prometheus=main`
      summary: >
        Trickster is down
  - alert: PrometheusRuleEvaluationFailures
    expr: increase(prometheus_rule_evaluation_failures_total[5m]) > 0
    labels:
      impact: negligible
      likelihood: certain
    annotations:
      polk_flant_com_markup_format: markdown
      description: |
        Pod {{$labels.pod}} in namespace {{$labels.namespace}} has failing rule evaluations.

        Please execute `kubectl -n {{$labels.namespace}} logs pod/{{$labels.pod}}` for more info.
      summary: >
        Prometheus has failing rule evaluations
