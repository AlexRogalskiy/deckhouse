- name: kubernetes.pod_status_incorrect
  rules:
    - alert: PodStatusIsIncorrect
      expr: >
        (count by (node, namespace, pod) (kube_pod_status_ready{condition="true"} == 0) * on (namespace, pod) group_left(node) (max by (namespace, node, pod) (kube_pod_info)))
        and
        (
          (count by (namespace, pod) (kube_pod_container_status_ready==1) * on (namespace, pod) group_left(node) (max by (namespace, node, pod) (kube_pod_info)))
          unless
          (count by (namespace, pod) (kube_pod_container_status_ready==0) * on (namespace, pod) group_left(node) (max by (namespace, node, pod) (kube_pod_info)))
        )
      for: 10m
      annotations:
        plk_markup_format: markdown
        plk_protocol_version: "1"
        plk_grouped_by__node_have_pods_with_incorrect_status: "NodeHavePodsWithIncorrectStatus,prometheus=deckhouse,node={{ $labels.node }}"
        description: |
          В кластере есть под {{ $labels.namespace }}/{{ $labels.pod }} на ноде {{ $labels.node }} у которого состояние пода NotReady, но у всех контейнеров данного пода статус Ready.
          Это может произойти из-за бага в [kubernetes](https://github.com/kubernetes/kubernetes/issues/80968).

          Что нужно сделать:
          1. Найти поды в таком состоянии: `kubectl get pod -o json --all-namespaces | jq '.items[] | select(.status.phase == "Running") | select(.status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | select(.status.conditions[] | select(.type == "Ready" and .status == "False")) | "\(.spec.nodeName)/\(.metadata.namespace)/\(.metadata.name)"'`
          2. Найти все проблемные ноды: `kubectl get pod -o json --all-namespaces | jq '.items[] | select(.status.phase == "Running") | select(.status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | select(.status.conditions[] | select(.type == "Ready" and .status == "False")) | .spec.nodeName' -r | sort | uniq -c`
          3. Перезапустить на данной доне `kubelet`: `systemctl restart kubelet`
        summary: Под {{ $labels.namespace }}/{{ $labels.pod }} на ноде {{ $labels.node }} находится в некорректном состоянии. Необходимо перезагрузить `kubelet`.

    - alert: NodeHavePodsWithIncorrectStatus
      expr: count by (node) (ALERTS{alertname="PodStatusIsIncorrect", alertstate="firing"}) > 0
      annotations:
        plk_markup_format: markdown
        plk_protocol_version: "1"
        plk_alert_type: "group"
        plk_grouped_by__cluster_have_nodes_with_incorrect_pod_status: "ClusterHaveNodesWithIncorrectPodStatus,prometheus=deckhouse"
        description: |
          В кластере есть нода {{ $labels.node }} на которой есть поды в некорректном статусе.
          Это может произойти из-за бага в [kubernetes](https://github.com/kubernetes/kubernetes/issues/80968).

          Что нужно сделать:
          1. Найти поды в таком состоянии: `kubectl get pod -o json --all-namespaces | jq '.items[] | select(.status.phase == "Running") | select(.status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | select(.status.conditions[] | select(.type == "Ready" and .status == "False")) | "\(.spec.nodeName)/\(.metadata.namespace)/\(.metadata.name)"'`
          2. Найти все проблемные ноды: `kubectl get pod -o json --all-namespaces | jq '.items[] | select(.status.phase == "Running") | select(.status.conditions[] | select(.type == "ContainersReady" and .status == "True")) | select(.status.conditions[] | select(.type == "Ready" and .status == "False")) | .spec.nodeName' -r | sort | uniq -c`
          3. Перезапустить на данной доне `kubelet`: `systemctl restart kubelet`
        summary: На ноде {{ $labels.node }} есть поды с некорректным статусом. Необходимо перезагрузить `kubelet`.

    - alert: ClusterHaveNodesWithIncorrectPodStatus
      expr: count(ALERTS{alertname="NodeHavePodsWithIncorrectStatus", alertstate="firing"}) > 0
      annotations:
        plk_markup_format: markdown
        plk_protocol_version: "1"
        plk_alert_type: "group"
        description: |
          В кластере есть ноды, на которых есть поды в некорректном статусе. На какой именно ноде проблема, можно узнать из связанного алерта.
          Это может произойти из-за бага в [kubernetes](https://github.com/kubernetes/kubernetes/issues/80968).
        summary: В кластере есть нода, на которой необходимо перезапустить kubelet.
