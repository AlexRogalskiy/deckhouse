# Модуль node-manager

## Функции модуля

1. **NodeGroup** — управление несколькими узлами как связанной группой:
    * Возможность определить label, annotation или taint, которые проставляются всем узлам группы,
    * <u>*Coming soon*</u>: Мониторинг группы как группы (группировка узлов на графиках по группам, группировка алертов
      о недоступности узлов, алерты о недоступности N узлов или N% узлов из группы),
2. **Chaos monkey** — инструмент систематического прерывания работы узлов для верификации отказоустойчивости
   элементов кластера и запущенных приложений.
3. **Managed node** — установка и настройка софта (docker, kubelet и пр.) на узле, подключение его в кластер и
   дальнейшее обновление.
    * Поддержка ubuntu 18.04 или centos 7 вне зависимости от типа используемой инфраструктуры
      (может быть абсолютно любое облако или абсолютно любое железо).
    * Подсистема представляет собой набор [идемпотентных шагов](../../candi/README.md) установки и настройки узла,
      которые:
        * осуществляют базовую настройку операционной системы (устанавливают проверенную версию ядра, отключают
          автообновления, устанавливают необходимые пакеты, конфигурируют журналирование и ротацию журналов, настраивают
          iptables и другие параметры безопасности);
        * настраивают Nginx (и систему автоматического обновления перечня upstream’ов) для балансировки запросов от узла
          (kubelet) по API серверам;
        * устанавливают и настраивают Docker и Kubernetes (и необходимые для них репозитории) и подключают узел к
          Kubernetes.
    * Версии Docker, ядра Linux, а так же полная версия Kubernetes (например, 1.16.8-0 для 1.16), определены в deckhouse
      и обновляются автоматически совместно с обновлением deckhouse.
    * Управление обновлениями узлов и их простоем (disruptions):
        * Автоматическое определение допустимой минорной версии Kubernetes (например, 1.16) для группы на основании ее
          собственных настроек (указанной для группы kubernetesVersion), версии по-умолчанию для всего кластера и текущей
          действительной версии control-plane (не допускается обновление узлов в опережение обновления control-plane).
        * Из группы одновременно производится обновление только одного узла и только если все узлы группы – доступны,
        * Обновления делятся на:
            * обычные – всегда происходят автоматически,
            * требующие disruption (например, обновление ядра, смена версии docker, значительная смена версии Kubelet и
              пр.) – можно выбрать ручной или автоматический режим.
        * В случае если разрешены автоматические disruptive обновления перед обновлением производится drain. 
    * Весь необходимый сопутствующий мониторинг (состояние и прогресс обновления). 
4. **Автомасштабирование кластера и заказ виртуальных машин в облаке**
    * Работает только в поддерживаемых облаках (см. подробнее модули `040-cloud-provider-*`).
    * Позволяет выбрать все необходимые параметры виртуальной машины (количество ресурсов, размер диска, настройки
      безопасности, подключаемые сети, подробнее см. конкретного провайдера).
    * Создание, запуск и подключение виртуальных машин к кластеру выполняется в полностью автоматическом режиме (в том
      числе благодаря использованию функционала **Managed node**).
    * Для группы можно как просто указать количество виртуальных машин (и оно будет поддерживаться), так и указать
      минимальное и максимальное значение (тогда будет работать автомасштабирование кластера).

## Конфигурация

### Включение модуля

Модуль по-умолчанию **включен**.

### Параметры

* `instancePrefix` — префикс, который следует использовать при создании instances в cloud provider.
  * Опциональный параметр.
  * Значение по-умолчанию может вычисляться из ClusterConfiguration, если кластер был установлен инсталлятором deckhouse (см.
    подробнее [документацию по установке](../../README.md#%D1%80%D0%B0%D0%B7%D0%B2%D0%BE%D1%80%D0%B0%D1%87%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B0-%D0%B8-%D1%83%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B0-deckhouse)).

#### Пример конфигурации

```yaml
nodeManager: |
  instancePrefix: kube
```

### NodeGroup custom resource

Ресурс описывает runtime параметры группы нод, которые будет использовать machine-controller-manager из этого модуля.

Все опции идут в `.spec`.

* `nodeType` — тип узлов, которые представляет эта группа.
  * Доступны следующие значения:
      * Cloud — узлы для этой группы будут автоматически создаваться (и удаляться) в настроенном облачном провайдере,
      * Static — статический узел, размещенный на железном сервере или виртуальной машине. Узел не управляется
        cloud-controller-manager'ом, даже если включен один из облачных провайдеров.
      * Hybrid – статический узел (созданный вручную или любыми внешними инструментами), размещенный в том же облаке, с
        которым настроена интеграция у одного из облачных провайдеров, на таком узле работает CSI и такой узел
        управляется cloud-controller-manager'ом (объект Node автоматически обогащается информацией о зоне и регионе по
        данным, полученным от облака; при удалении узла из облака, соответствующий ему Node-объект будет
        удален в Kubernetes).
* `allowDisruptions` — автоматически выдавать разрешения на обновления требующие возможный простой (disruption).
  * Формат — boolean.
  * По-умолчанию, `true`.
* `kubernetesVersion` — желаемая minor-версия Kubernetes.
  * Например, `1.16`.
  * По-умолчанию соответствует глобально выбранной для кластера версии (см. подробнее [документацию по установке]((../../README.md#%D1%80%D0%B0%D0%B7%D0%B2%D0%BE%D1%80%D0%B0%D1%87%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B0-%D0%B8-%D1%83%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B0-deckhouse)))
    или, если таковая не определена, текущей версии control-plane'а.
* `static` — параметры связанные со статическими узлами.
  * **Внимание!** Допустимо использовать только совместно с `nodeType: Static`
  * `internalNetworkCIDRs` — список подсетей, использующиеся для коммуникации внутри кластера. На основании этого списка
    производится автоматическое определение InternalIP узла и адреса, на котором будет слушать kubelet. 
    * Формат — массив строк. Subnet CIDR.
    * Пример:

      ```yaml
      internalNetworkCIDRs:
      - "10.2.2.3/24"
      - "10.1.1.1/24"
      ```
* `cloudInstances` – параметры заказа облачных виртуальных машин.
  * **Внимание!** Допустимо использовать только совместно с `nodeType: Cloud`
  * `classReference` – ссылка на объект InstanceClass. Уникален для каждого `cloud-provider-` модуля.
    * `kind` — тип объекта (например, `OpenStackInstanceClass`). Тип объекта указан в документации соответствующего
      `cloud-provider-` модуля.
    * `name` — имя нужного InstanceClass объекта (например, `finland-medium`).
  * `maxPerZone` — максимальное количество инстансов в зоне. Проставляется как верхняя граница в cluster-autoscaler.
  * `minPerZone` — минимальное количество инстансов в зоне. Проставляется в объект MachineDeployment и в качестве нижней
     границы в cluster-autoscaler.
    * **Внимание!** Не может быть меньше 1. 
  * `maxUnavailablePerZone` — сколько инстансов может быть недоступно при RollingUpdate'е.
    * По-умолчанию `0`.
  * `maxSurgePerZone` — сколько инстансов создавать одновременно при scale-up.
    * По-умолчанию `1`.
  * `zones` — переопределение перечня зон, в которых создаются инстансы.
    * Формат — массив строк.
    * Опциональный параметр.
    * Значение по-умолчанию зависит от выбранного облачного провайдера и обычно соответствует всем зонам используемого
      региона.
* `nodeTemplate` — настройки Node объектов в Kubernetes, которые будут добавлены после регистрации ноды.
  * `labels` — аналогично стандартному [полю](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#objectmeta-v1-meta) `metadata.labels`
    * Пример:

      ```yaml
      labels:
        environment: production
        app: warp-drive-ai

  * `annotations` — аналогично стандартному [полю](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#objectmeta-v1-meta) `metadata.annotations`
    * Пример:

      ```yaml
      annotations:
        ai.fleet.com/discombobulate: "true"
      ```

  * `taints` — аналогично полю `.spec.taints` из объекта [Node](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#taint-v1-core). **Внимание!** Доступны только поля `effect`, `key`, `values`.
    * Пример:

      ```yaml
      taints:
      - effect: NoExecute
        key: ship-class
        value: frigate
      ```

* `chaos` — настройки chaos monkey:
  * Опциональный параметр.
  * `mode` — режим работы chaos monkey, возможные значения: `DrainAndDelete` — при срабатывании drain'ит и удаляет ноду, `Disabled` — не трогает данную NodeGroup.
    * По-умолчанию `Disabled`.
  * `period` — в какой интервал времени сработает chaos monkey (указывать можно в [golang формате](https://golang.org/pkg/time/#ParseDuration));
    * По-умолчанию `6h`.

#### Пример NodeGroup

```yaml
apiVersion: deckhouse.io/v1alpha1
kind: NodeGroup
metadata:
  name: test
spec:
  nodeType: Cloud
  kubernetesVersion: "1.16"
  cloudInstances:
    zones:
    - eu-west-1a
    - eu-west-1b
    minPerZone: 1
    maxPerZone: 3
    maxUnavailablePerZone: 0
    maxSurgePerZone: 1
    classReference:
      kind: AWSInstanceClass
      name: test
  nodeTemplate:
    labels:
      environment: production
      app: warp-drive-ai
    annotations:
      ai.fleet.com/discombobulate: "true"
    taints:
    - effect: NoExecute
      key: ship-class
      value: frigate
  chaos:
    mode: DrainAndReboot
    period: 24h
  allowDisruptions: false
```

## Как мне перекатить машины с новой конфигурацией?

При изменении конфигурации Deckhouse (как в этом модуле, так и в любом из облачных провайдеров) виртуальные машины не будут перезаказаны. Перекат происходит только после изменения `InstanceClass` или `NodeGroup` объектов.

Для того, чтобы форсированно перекатить все Machines, следует добавить/изменить аннотацию `manual-rollout-id` в `NodeGroup`: `kubectl annotate NodeGroup имя_ng "manual-rollout-id=$(uuidgen)" --overwrite`.
