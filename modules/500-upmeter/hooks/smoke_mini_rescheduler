#!/bin/bash

source /deckhouse/shell_lib.sh

function __config__() {
  cat <<EOF
    configVersion: v1
    kubernetes:
    - name: nodes
      group: main
      keepFullObjectsInMemory: false
      queue: /modules/$(module::name::kebab_case)/update_selector
      executeHookOnEvent: ["Modified", "Deleted"]
      apiVersion: v1
      kind: Node
      jqFilter: |
          {
            "hostname": .metadata.labels."kubernetes.io/hostname",
            "zone": (.metadata.labels | if has("failure-domain.beta.kubernetes.io/zone") then ."failure-domain.beta.kubernetes.io/zone" else "NONE" end),
            "unschedulable": (.spec.unschedulable // false),
            "ready": ([(.status.conditions // [])[] | select(.type == "Ready" and .status == "True")] | length > 0)
          }
    - name: statefulsets
      group: main
      keepFullObjectsInMemory: false
      queue: /modules/$(module::name::kebab_case)/update_selector
      apiVersion: apps/v1
      kind: StatefulSet
      namespace:
        nameSelector:
          matchNames: ["d8-upmeter"]
      labelSelector:
        matchLabels:
          app: smoke-mini
      executeHookOnEvent: []
      jqFilter: |
        {
          "nodeHostname": (.metadata.annotations | if has("node") then .node else "" end),
          "zone": (.metadata.annotations | if has("zone") then .zone else "NONE" end),
          "stsIndex": .metadata.name[-1:],
          "image": .spec.template.spec.containers[0].image,
          "storageClassName": (((.spec.volumeClaimTemplates // [])[] | select(.metadata.name=="disk").spec.storageClassName) // false)
        }
    - name: pods
      group: main
      keepFullObjectsInMemory: false
      queue: /modules/$(module::name::kebab_case)/update_selector
      apiVersion: v1
      kind: Pod
      namespace:
        nameSelector:
          matchNames: ["d8-upmeter"]
      labelSelector:
        matchLabels:
          app: smoke-mini
      executeHookOnEvent: []
      jqFilter: |
        {
          "name": .metadata.name,
          "node": .spec.nodeName,
          "stsIndex": .metadata.ownerReferences[0].name[-1:],
          "creationTimestamp": (.metadata.creationTimestamp | fromdateiso8601)
        }
    - name: pdb
      group: main
      keepFullObjectsInMemory: false
      queue: /modules/$(module::name::kebab_case)/update_selector
      apiVersion: policy/v1beta1
      kind: PodDisruptionBudget
      namespace:
        nameSelector:
          matchNames: ["d8-upmeter"]
      labelSelector:
        matchLabels:
          app: smoke-mini
      executeHookOnEvent: []
      jqFilter: |
        {
          "disruptionsAllowed": (.status.disruptionsAllowed > 0)
        }
    - name: default_sc
      group: main
      keepFullObjectsInMemory: true
      queue: /modules/$(module::name::kebab_case)/update_selector
      apiVersion: storage.k8s.io/v1
      kind: Storageclass
      jqFilter: |
        (.metadata.annotations."storageclass.beta.kubernetes.io/is-default-class" == "true") or
        (.metadata.annotations."storageclass.kubernetes.io/is-default-class" == "true")

    # pvc_modified
    - name: pvc_modified
      keepFullObjectsInMemory: true
      queue: /modules/$(module::name::kebab_case)/update_selector_pvc
      executeHookOnEvent: ["Modified"]
      executeHookOnSynchronization: false
      group: pvc_modified
      apiVersion: v1
      kind: PersistentVolumeClaim
      namespace:
        nameSelector:
          matchNames: ["d8-upmeter"]
      labelSelector:
        matchLabels:
          app: smoke-mini
      jqFilter: |
        {
          "name": .metadata.name,
          "isDeleted": (if .metadata | has("deletionTimestamp") then true else false end)
        }
    - name: pods_mod
      keepFullObjectsInMemory: true
      queue: /modules/$(module::name::kebab_case)/update_selector_pvc
      group: pvc_modified
      apiVersion: v1
      kind: Pod
      namespace:
        nameSelector:
          matchNames: ["d8-upmeter"]
      labelSelector:
        matchLabels:
          app: smoke-mini
      executeHookOnEvent: []
      executeHookOnSynchronization: false
      jqFilter: |
        {
          "name": .metadata.name,
          "isPending": (if .status.phase == "Pending" then true else false end)
        }
    # pvc_deleted
    - name: pvc_deleted
      keepFullObjectsInMemory: true
      queue: /modules/$(module::name::kebab_case)/update_selector_pvc
      executeHookOnEvent: ["Deleted"]
      executeHookOnSynchronization: false
      group: pvc_deleted
      apiVersion: v1
      kind: PersistentVolumeClaim
      namespace:
        nameSelector:
          matchNames: ["d8-upmeter"]
      labelSelector:
        matchLabels:
          app: smoke-mini
      jqFilter: |
        {
          "name": .metadata.name,
          "isDeleted": (if .metadata | has("deletionTimestamp") then true else false end)
        }
    - name: pods_del
      group: pvc_deleted
      keepFullObjectsInMemory: true
      queue: /modules/$(module::name::kebab_case)/update_selector_pvc
      apiVersion: v1
      kind: Pod
      namespace:
        nameSelector:
          matchNames: ["d8-upmeter"]
      labelSelector:
        matchLabels:
          app: smoke-mini
      executeHookOnEvent: []
      executeHookOnSynchronization: false
      jqFilter: |
        {
          "name": .metadata.name,
          "isPending": (if .status.phase == "Pending" then true else false end)
        }
    schedule:
    - group: main
      queue: /modules/$(module::name::kebab_case)/update_selector
      crontab: "* * * * *"
EOF
}

function __on_group::pvc_modified() {
  for i in $(context::jq -r '.snapshots.pvc_modified | keys[]'); do
    if context::jq -er --argjson i "$i" '.snapshots.pvc_modified[$i].filterResult.isDeleted' >/dev/null; then
      pvc_name="$(context::jq -er --argjson i "$i" '.snapshots.pvc_modified[$i].filterResult.name')"
      pod_name="$(echo "$pvc_name" | cut -d- -f2-)"
      if context::jq -er --arg pod_name "$pod_name" '.snapshots.pods_mod[].filterResult | select(.name == $pod_name)' >/dev/null; then
        kubernetes::delete_if_exists::non_blocking "d8-upmeter" "pod/$pod_name"
        echo "!!! NOTICE: deleting pod/$pod_name because persistentvolumeclaim/$pvc_name is Terminating !!!"
      fi
    fi
  done
}

function __on_group::pvc_deleted() {
  for i in $(context::jq -r '.snapshots.pods_del | keys[]'); do
    if context::jq -er --argjson i "$i" '.snapshots.pods_del[$i].filterResult.isPending' >/dev/null; then
      pod_name="$(context::get snapshots.pods_del.$i.filterResult.name)"
      pvc_name="disk-$pod_name"
      kubernetes::delete_if_exists::non_blocking "d8-upmeter" "pod/$pod_name"
      echo "!!! NOTICE: deleting pod/$pod_name because persistentvolumeclaim/$pvc_name was deleted !!!"
    fi
  done
}

# schedule one sts
function __on_group::main() {
  effective_storage_class="false"
  if context::jq -er '.snapshots.default_sc[] | select(.filterResult == true)' >/dev/null; then
    effective_storage_class="$(context::jq -r '[.snapshots.default_sc[] | select(.filterResult == true)] | first | .object.metadata.name')"
  fi
  if values::has --config global.storageClass; then
    effective_storage_class="$(values::get --config global.storageClass)"
  fi
  if values::has --config upmeter.smokeMini.storageClass; then
    effective_storage_class="$(values::get --config upmeter.smokeMini.storageClass)"
  fi

  if ! values::has upmeter.internal.smokeMini.sts.a.image; then
    current_values="$(values::get upmeter.internal.smokeMini.sts)"
    values="$(context::jq -rc --argjson current_values "$current_values" '
      [
        .snapshots.statefulsets[].filterResult
        | {
            (.stsIndex): {
              "node": .nodeHostname,
              "zone": .zone,
              "image": .image,
              "effectiveStorageClass": (.storageClassName // false)
            }
          }
      ]
      | reduce .[] as $i ($current_values; . + $i)
    ')"
    values::set upmeter.internal.smokeMini.sts "$values"
  fi

  ### Choose sts/pod to reschedule

  # Find non existent sts, if any
  target_sts_index="$(values::jq -r '.upmeter.internal.smokeMini.sts | [to_entries[] | select(.value | has("node") | not)] | select(.!=[]) | first.key')"

  # Otherwise, find sts placed on non existent node
  if [ -z "$target_sts_index" ]; then
    for i in $(context::jq -r '.snapshots.statefulsets | keys[]'); do
      if ! context::jq -e --argjson i "$i" '.snapshots.statefulsets[$i].filterResult.nodeHostname as $node_hostname | .snapshots.nodes[].filterResult | select(.hostname == $node_hostname)' >/dev/null; then
        target_sts_index="$(context::jq -r --argjson i "$i" '.snapshots.statefulsets[$i].filterResult.stsIndex')"
        break
      fi
    done
  fi

  # Otherwise, find sts placed on unscheduleable node
  if [ -z "$target_sts_index" ]; then
    for i in $(context::jq -r '.snapshots.statefulsets | keys[]'); do
      if context::jq -e --argjson i "$i" '.snapshots.statefulsets[$i].filterResult as $sts | .snapshots.nodes[].filterResult | select(.hostname == $sts.nodeHostname and .unschedulable)' >/dev/null; then
        target_sts_index="$(context::jq -r --argjson i "$i" '.snapshots.statefulsets[$i].filterResult.stsIndex')"
        break
      fi
    done
  fi

  # Otherwise, find sts with not up-to-date storage class
  if [ -z "$target_sts_index" ]; then
    for i in $(context::jq -r '.snapshots.statefulsets | keys[]'); do
      if context::jq -e --argjson i "$i" --arg effective_storage_class "$effective_storage_class" '.snapshots.statefulsets[$i].filterResult | select(.storageClassName | tostring != $effective_storage_class)' >/dev/null; then
        target_sts_index="$(context::jq -r --argjson i "$i" '.snapshots.statefulsets[$i].filterResult.stsIndex')"
        break
      fi
    done
  fi

  # Otherwise, find sts which was not moved longest time
  if [ -z "$target_sts_index" ]; then
    # Check that PDB allows us to make a step
    if ! context::jq -e '.snapshots.pdb[].filterResult.disruptionsAllowed' >/dev/null; then
      return 0
    fi

    target_sts_index="$(context::jq -r '[.snapshots.pods[].filterResult] | min_by(.creationTimestamp).stsIndex')"

    # Ignore, if it was moved less than N-1 cron schedule periods ago (where N is number of sts)
    pod_creation_timestamp="$(context::jq -r '[.snapshots.pods[].filterResult] | min_by(.creationTimestamp).creationTimestamp')"
    unixtime_now="$(date +%s)"
    n="$(values::jq -r '.upmeter.internal.smokeMini.sts | [to_entries[] | select(.value | has("node"))] | length')"
    if ((unixtime_now - pod_creation_timestamp < 60 * (n - 1))); then
      return 0
    fi
  fi

  echo "LOG: target_sts_index: $target_sts_index"

  ### Choose new target node for the pod

  # Calculate the amount of statefulsets in zones
  zones_map=$(
    context::jq -r '
    (
      [.snapshots.nodes[].filterResult.zone]
      | unique
      | reduce .[] as $i ({}; . + {$i: 0})
    ) as $zone_map
    | [.snapshots.statefulsets[].filterResult]
    | reduce .[] as $i ($zone_map; .[$i.zone] += 1)
    | [to_entries[] | {"name": .key, "sts_count": .value}]'
  )

  # Get sts zone
  sts_zone="$(context::jq -r --arg sts "$target_sts_index" '.snapshots.statefulsets[].filterResult | select(.stsIndex == $sts).zone')"

  # Check, if zone still exists
  if context::jq -er --arg sts_zone "$sts_zone" '.snapshots.nodes | [.[] | select(.filterResult.zone == $sts_zone and .filterResult.ready == true and .filterResult.unschedulable == false)] | length < 1' >/dev/null; then
    sts_zone=""
  fi

  # Check sts zone
  if [ -n "$sts_zone" ]; then
    # zone is set, checking constraints

    # Assumption: the number of zones is NOT bigger than the number of
    # statefulsets (5 for now). So each zone must be covered by at least one
    # statefulset.

    # if sts count > 1 in zone
    if jq -er --arg sts_zone "$sts_zone" '.[] | select(.name == $sts_zone).sts_count > 1' <<<"$zones_map" >/dev/null; then
      zones_without_sts="$(context::jq -r '([.snapshots.nodes[].filterResult.zone] | unique) - ([.snapshots.statefulsets[].filterResult.zone] | unique) | .[]')"

      # If there are zones without sts, then select new zone, because we want to
      # check network and disks in every zone
      if [ -n "$zones_without_sts" ]; then
        sts_zone="$(sort -R <<<"$zones_without_sts" | sed -n '1p')"
      fi
    fi
  else
    # Zone is not set, select new zone
    zones_with_min_sts="$(jq -r 'group_by(.sts_count)[0][].name' <<<"$zones_map")"
    sts_zone="$(sort -R <<<"$zones_with_min_sts" | sed -n '1p')"
  fi

  # Count sts per node (in zone)
  nodes_map="$(context::jq -r --arg zone "$sts_zone" '
      (
        [
          .snapshots.nodes[].filterResult
          | select(.zone == $zone and .ready == true and .unschedulable == false)
          | .hostname
        ]
        | unique
        | reduce .[] as $i ({}; . + {$i: 0})
      ) as $node_map
      | [.snapshots.statefulsets[].filterResult | select(.zone == $zone)]
      | reduce .[] as $i ($node_map; .[$i.nodeHostname] += 1)
      | [to_entries[] | {"name": .key, "sts_count": .value}]
  ')"

  # Select new node
  current_node="$(context::jq -r --arg podName "smoke-mini-${target_sts_index}-0" '.snapshots.pods[].filterResult | select(.name == $podName) | .node' | sed -n '1p')"
  nodes_with_min_sts="$(jq -r 'group_by(.sts_count)[0][].name' <<<"$nodes_map")"

  # Select new node excluding current node if the pod is scheduled.
  # Using jq because "grep -v" fails with exit code 1.
  sts_node="$(sort -R <<<"$nodes_with_min_sts" | jq -Rr --arg current "$current_node" '[select (. != $current)][0] // ""' | sed -n '1p')"

  ### Apply values

  # Apply new image
  values::set upmeter.internal.smokeMini.sts.${target_sts_index}.image "$(values::get global.modulesImages.registry):$(values::get global.modulesImages.tags.upmeter.smokeMini)"

  if [ -z "$sts_node" ]; then
    # No new node to assign, hence no zone to change, exiting.
    return 0
  fi

  # Apply new node
  values::set upmeter.internal.smokeMini.sts.${target_sts_index}.node "$sts_node"

  ### Statefulsets sequential update

  if [ "$(values::get upmeter.internal.smokeMini.sts.${target_sts_index}.zone)" != "$sts_zone" ]; then
    kubernetes::delete_if_exists::non_blocking "d8-upmeter" "persistentvolumeclaim/disk-smoke-mini-${target_sts_index}-0"
    echo "!!! NOTICE: zone changed, deleting persistentvolumeclaim/disk-smoke-mini-${target_sts_index}-0 !!!"
    values::set upmeter.internal.smokeMini.sts.${target_sts_index}.zone "\"$sts_zone\""
  fi

  storage_class_from_sts="$(values::get upmeter.internal.smokeMini.sts.${target_sts_index}.effectiveStorageClass)"
  if [ "$storage_class_from_sts" != "$effective_storage_class" ]; then
    values::set upmeter.internal.smokeMini.sts.${target_sts_index}.effectiveStorageClass "$effective_storage_class"
    if [ -n "$storage_class_from_sts" ]; then
      kubernetes::delete_if_exists::non_blocking "d8-upmeter" "persistentvolumeclaim/disk-smoke-mini-${target_sts_index}-0"
      echo "!!! NOTICE: storage class changed, deleting persistentvolumeclaim/disk-smoke-mini-${target_sts_index}-0 !!!"
      kubernetes::delete_if_exists::non_blocking "d8-upmeter" "statefulset/smoke-mini-$target_sts_index"
      echo "!!! NOTICE: storage class changed, deleting statefulset/smoke-mini-$target_sts_index !!!"
    fi
  fi
}

hook::run "$@"
