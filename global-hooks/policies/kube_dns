#!/bin/bash

source /antiopa/shell_lib.sh

function __config__() {
  jo -p onStartup=520 onKubernetesEvent="$(jo -a \
    "$(jo kind=node jqFilter=".metadata.labels" disableDebug=true)" \
  )"
}



function __main__() {
  fltr='include "remove_empty"; .'

  count_system_nodes=$(cluster::count_nodes_by_role "system")
  count_kube_dns_nodes=$(cluster::count_nodes_by_role "kubeDns")

  ###
  # Политика #1: kube-dns должен выполняться на system-узлах, если они есть

  # Добавляем toleration, если не добавлен
  if (( "$count_system_nodes" > 0 || "$count_kube_dns_nodes" > 0 )); then
      fltr=$fltr' | .spec.template.spec.tolerations = ([{"key":"dedicated.flant.com","operator":"Equal","value": "kube-dns"},{"key":"dedicated.flant.com","operator":"Equal","value": "system"},{"key":"node-role/system"},{"key":"node-role.kubernetes.io/master"}] | unique)'
    if (( "$count_kube_dns_nodes" > 0 )); then
      fltr=$fltr' | .spec.template.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms = [{"matchExpressions": [{"key":"node-role.flant.com/kube-dns","operator":"Exists"}]}]'
    elif (( "$count_system_nodes" > 0 )); then
      fltr=$fltr' | .spec.template.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms = [{"matchExpressions": [{"key":"node-role.flant.com/system","operator":"Exists"}]}]'
    else
      # Удаляем из каждого nodeSelectorTerm по matchExpression, если он был добавлен
      fltr=$fltr' | del(.spec.template.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms)'
    fi
  fi

  ###
  # Политика #2: Количество реплик kube-dns
  #  * В AWS и GCE — работает kube-dns-autoscaler (который ставит kops, и мы не хотим с ним спорить)
  #  * В других случаях:
  #    * Если есть system-узлы, то kube-dns должен выполняться на каждом system-узле
  #      * если системный узел один, то количество реплик должно равняться 2, чтобы избежать перебоев сервиса при перекате kube-dns
  #    * Иначе количество реплик должно быть:
  #      * равно 2 или больше (если руками кто-то поставил),
  #      * но не больше, чем количество неспецифичных нод в кластере (без наших специальных taint)
  cluster_type=$(values::get --required global.discovery.clusterType)
  if [[ "$cluster_type" != "AWS" && "$cluster_type" != "GCE" ]] ; then
    if (( "$count_kube_dns_nodes" > 1 )); then
      fltr=$fltr' | .spec.replicas = '$count_kube_dns_nodes
    elif (( "$count_kube_dns_nodes" == 1 )); then
      fltr=$fltr' | .spec.replicas = 2'
    elif (( "$count_system_nodes" > 1 )); then
      fltr=$fltr' | .spec.replicas = '$count_system_nodes
    elif (( "$count_system_nodes" == 1 )); then
      fltr=$fltr' | .spec.replicas = 2'
    else
      count_nonspecific_nodes=$(cluster::nonspecific_nodes | wc -l)
      fltr=$fltr' | .spec.replicas |= ([[2,.] | max, '$count_nonspecific_nodes'] | min)'
    fi
  fi


  ###
  # Политика #3: Не нужно запускать больше одного kube-dns на узле
  fltr=$fltr' | .spec.template.spec.affinity.podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution = ([{"weight":100,"podAffinityTerm":{"labelSelector":{"matchLabels":{"k8s-app":"kube-dns"}},"topologyKey":"kubernetes.io/hostname"}}] | unique)'

  # Определяем имя deployment'а (в Azure он называется не kube-dns)
  kube_dns_deployment_name=$(kubectl -n kube-system get deployment -l k8s-app=kube-dns -o name)

  # Удаляем пустые поля в affinity
  fltr=$fltr' | .spec.template.spec.affinity |= remove_empty'

  # Применяем
  kubectl::jq_patch kube-system ${kube_dns_deployment_name} "$fltr"
}

hook::run $@
